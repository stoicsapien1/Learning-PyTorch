{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4acee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa98c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90e4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id', 'Unnamed: 32'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2960c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           M        17.99         10.38          122.80     1001.0   \n",
       "1           M        20.57         17.77          132.90     1326.0   \n",
       "2           M        19.69         21.25          130.00     1203.0   \n",
       "3           M        11.42         20.38           77.58      386.1   \n",
       "4           M        20.29         14.34          135.10     1297.0   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564         M        21.56         22.39          142.00     1479.0   \n",
       "565         M        20.13         28.25          131.20     1261.0   \n",
       "566         M        16.60         28.08          108.30      858.1   \n",
       "567         M        20.60         29.33          140.10     1265.0   \n",
       "568         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f451ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc43ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14c21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd71fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= encoder.fit_transform(y_train)\n",
    "\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea3bd0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad0310d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add10fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([114, 30])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d6a9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca0fdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb307eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "    def __init__(self,X):\n",
    "\n",
    "        self.weights = torch.rand(X.shape[1],1,dtype=torch.float64,requires_grad=True)\n",
    "        self.bias = torch.zeros(1,dtype=torch.float64,requires_grad=True)\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        z = torch.matmul(X,self.weights) + self.bias\n",
    "        y_pred = torch.sigmoid(z)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def loss_function(self,y_pred,y):\n",
    "\n",
    "        epsilon= 1e-7\n",
    "        y_pred = torch.clamp(y_pred,epsilon,1-epsilon)\n",
    "\n",
    "        loss = -(y * torch.log(y_pred) + (1-y)*torch.log(1-y_pred)).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e87014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.25\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36d4a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "913b7be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1,Loss : 3.718703023079527\n",
      "Epoch:2,Loss : 3.422909204424197\n",
      "Epoch:3,Loss : 3.1061040888668905\n",
      "Epoch:4,Loss : 2.7587758566216865\n",
      "Epoch:5,Loss : 2.3642921785548894\n",
      "Epoch:6,Loss : 1.956753579956398\n",
      "Epoch:7,Loss : 1.5876546204537392\n",
      "Epoch:8,Loss : 1.297048312974071\n",
      "Epoch:9,Loss : 1.0877738295178283\n",
      "Epoch:10,Loss : 0.9628133788911177\n",
      "Epoch:11,Loss : 0.8995563467435035\n",
      "Epoch:12,Loss : 0.8637564842752475\n",
      "Epoch:13,Loss : 0.8374267581148488\n",
      "Epoch:14,Loss : 0.8153514282703195\n",
      "Epoch:15,Loss : 0.7963020266242521\n",
      "Epoch:16,Loss : 0.7798556569378108\n",
      "Epoch:17,Loss : 0.7657239659509534\n",
      "Epoch:18,Loss : 0.7536447191550107\n",
      "Epoch:19,Loss : 0.7433699573278842\n",
      "Epoch:20,Loss : 0.7346659149737657\n",
      "Epoch:21,Loss : 0.7273129617499173\n",
      "Epoch:22,Loss : 0.7211063622493197\n",
      "Epoch:23,Loss : 0.7158590194333325\n",
      "Epoch:24,Loss : 0.7114051208274336\n",
      "Epoch:25,Loss : 0.7076025708017912\n",
      "Epoch:26,Loss : 0.7043330691916092\n",
      "Epoch:27,Loss : 0.7015001355824212\n",
      "Epoch:28,Loss : 0.6990260343962805\n",
      "Epoch:29,Loss : 0.6968484604533487\n",
      "Epoch:30,Loss : 0.6949174915644611\n",
      "Epoch:31,Loss : 0.6931930119718234\n",
      "Epoch:32,Loss : 0.6916426323170531\n",
      "Epoch:33,Loss : 0.6902400515690648\n",
      "Epoch:34,Loss : 0.6889637821533219\n",
      "Epoch:35,Loss : 0.6877961614828575\n",
      "Epoch:36,Loss : 0.6867225846289473\n",
      "Epoch:37,Loss : 0.6857309061683304\n",
      "Epoch:38,Loss : 0.6848109711902378\n",
      "Epoch:39,Loss : 0.6839542451508901\n",
      "Epoch:40,Loss : 0.6831535197643059\n",
      "Epoch:41,Loss : 0.6824026777694933\n",
      "Epoch:42,Loss : 0.6816965036193869\n",
      "Epoch:43,Loss : 0.6810305302532359\n",
      "Epoch:44,Loss : 0.680400914425955\n",
      "Epoch:45,Loss : 0.6798043347907067\n",
      "Epoch:46,Loss : 0.6792379082232873\n",
      "Epoch:47,Loss : 0.6786991208540146\n",
      "Epoch:48,Loss : 0.6781857710179107\n",
      "Epoch:49,Loss : 0.6776959219070379\n",
      "Epoch:50,Loss : 0.6772278621533102\n",
      "Epoch:51,Loss : 0.6767800729175519\n",
      "Epoch:52,Loss : 0.6763512003341944\n",
      "Epoch:53,Loss : 0.6759400323779378\n",
      "Epoch:54,Loss : 0.6755454793917194\n",
      "Epoch:55,Loss : 0.6751665576541066\n",
      "Epoch:56,Loss : 0.6748023754760477\n",
      "Epoch:57,Loss : 0.6744521214074298\n",
      "Epoch:58,Loss : 0.6741150542074323\n",
      "Epoch:59,Loss : 0.6737904942926183\n",
      "Epoch:60,Loss : 0.6734778164257398\n",
      "Epoch:61,Loss : 0.6731764434484478\n",
      "Epoch:62,Loss : 0.6728858408941593\n",
      "Epoch:63,Loss : 0.6726055123445708\n",
      "Epoch:64,Loss : 0.672334995415814\n",
      "Epoch:65,Loss : 0.6720738582788615\n",
      "Epoch:66,Loss : 0.6718216966342175\n",
      "Epoch:67,Loss : 0.671578131073754\n",
      "Epoch:68,Loss : 0.6713428047731972\n",
      "Epoch:69,Loss : 0.6711153814676676\n",
      "Epoch:70,Loss : 0.6708955436700753\n",
      "Epoch:71,Loss : 0.6706829910983759\n",
      "Epoch:72,Loss : 0.670477439282874\n",
      "Epoch:73,Loss : 0.6702786183291147\n",
      "Epoch:74,Loss : 0.6700862718155541\n",
      "Epoch:75,Loss : 0.6699001558082701\n",
      "Epoch:76,Loss : 0.6697200379775686\n",
      "Epoch:77,Loss : 0.6695456968035204\n",
      "Epoch:78,Loss : 0.669376920859314\n",
      "Epoch:79,Loss : 0.669213508162873\n",
      "Epoch:80,Loss : 0.6690552655885121\n",
      "Epoch:81,Loss : 0.6689020083315373\n",
      "Epoch:82,Loss : 0.6687535594196498\n",
      "Epoch:83,Loss : 0.6686097492658384\n",
      "Epoch:84,Loss : 0.6684704152581402\n",
      "Epoch:85,Loss : 0.6683354013822527\n",
      "Epoch:86,Loss : 0.6682045578734861\n",
      "Epoch:87,Loss : 0.6680777408949976\n",
      "Epoch:88,Loss : 0.6679548122396213\n",
      "Epoch:89,Loss : 0.6678356390529344\n",
      "Epoch:90,Loss : 0.6677200935754922\n",
      "Epoch:91,Loss : 0.6676080529024013\n",
      "Epoch:92,Loss : 0.667499398758616\n",
      "Epoch:93,Loss : 0.6673940172885309\n",
      "Epoch:94,Loss : 0.6672917988585993\n",
      "Epoch:95,Loss : 0.6671926378718519\n",
      "Epoch:96,Loss : 0.6670964325933079\n",
      "Epoch:97,Loss : 0.667003084985386\n",
      "Epoch:98,Loss : 0.6669125005525081\n",
      "Epoch:99,Loss : 0.6668245881941796\n",
      "Epoch:100,Loss : 0.6667392600658985\n",
      "Epoch:101,Loss : 0.6666564314473111\n",
      "Epoch:102,Loss : 0.66657602061709\n",
      "Epoch:103,Loss : 0.6664979487340577\n",
      "Epoch:104,Loss : 0.6664221397241299\n",
      "Epoch:105,Loss : 0.6663485201726842\n",
      "Epoch:106,Loss : 0.6662770192220007\n",
      "Epoch:107,Loss : 0.6662075684734514\n",
      "Epoch:108,Loss : 0.6661401018941432\n",
      "Epoch:109,Loss : 0.6660745557277418\n",
      "Epoch:110,Loss : 0.6660108684092316\n",
      "Epoch:111,Loss : 0.6659489804833826\n",
      "Epoch:112,Loss : 0.665888834526713\n",
      "Epoch:113,Loss : 0.665830375072758\n",
      "Epoch:114,Loss : 0.6657735485404609\n",
      "Epoch:115,Loss : 0.6657183031655272\n",
      "Epoch:116,Loss : 0.6656645889345818\n",
      "Epoch:117,Loss : 0.665612357521993\n",
      "Epoch:118,Loss : 0.665561562229225\n",
      "Epoch:119,Loss : 0.6655121579265995\n",
      "Epoch:120,Loss : 0.6654641009973479\n",
      "Epoch:121,Loss : 0.6654173492838489\n",
      "Epoch:122,Loss : 0.6653718620359454\n",
      "Epoch:123,Loss : 0.6653275998612524\n",
      "Epoch:124,Loss : 0.66528452467736\n",
      "Epoch:125,Loss : 0.6652425996658505\n",
      "Epoch:126,Loss : 0.6652017892280517\n",
      "Epoch:127,Loss : 0.6651620589424494\n",
      "Epoch:128,Loss : 0.6651233755236858\n",
      "Epoch:129,Loss : 0.6650857067830852\n",
      "Epoch:130,Loss : 0.6650490215906342\n",
      "Epoch:131,Loss : 0.6650132898383626\n",
      "Epoch:132,Loss : 0.6649784824050651\n",
      "Epoch:133,Loss : 0.664944571122311\n",
      "Epoch:134,Loss : 0.6649115287416889\n",
      "Epoch:135,Loss : 0.6648793289032392\n",
      "Epoch:136,Loss : 0.6648479461050261\n",
      "Epoch:137,Loss : 0.6648173556738041\n",
      "Epoch:138,Loss : 0.6647875337367395\n",
      "Epoch:139,Loss : 0.6647584571941422\n",
      "Epoch:140,Loss : 0.6647301036931725\n",
      "Epoch:141,Loss : 0.6647024516024821\n",
      "Epoch:142,Loss : 0.6646754799877588\n",
      "Epoch:143,Loss : 0.6646491685881354\n",
      "Epoch:144,Loss : 0.6646234977934344\n",
      "Epoch:145,Loss : 0.6645984486222158\n",
      "Epoch:146,Loss : 0.6645740027005965\n",
      "Epoch:147,Loss : 0.6645501422418167\n",
      "Epoch:148,Loss : 0.6645268500265211\n",
      "Epoch:149,Loss : 0.6645041093837321\n",
      "Epoch:150,Loss : 0.6644819041724873\n",
      "Epoch:151,Loss : 0.6644602187641192\n",
      "Epoch:152,Loss : 0.6644390380251507\n",
      "Epoch:153,Loss : 0.6644183473007874\n",
      "Epoch:154,Loss : 0.6643981323989824\n",
      "Epoch:155,Loss : 0.6643783795750536\n",
      "Epoch:156,Loss : 0.6643590755168353\n",
      "Epoch:157,Loss : 0.6643402073303412\n",
      "Epoch:158,Loss : 0.6643217625259242\n",
      "Epoch:159,Loss : 0.6643037290049131\n",
      "Epoch:160,Loss : 0.6642860950467087\n",
      "Epoch:161,Loss : 0.6642688492963252\n",
      "Epoch:162,Loss : 0.6642519807523585\n",
      "Epoch:163,Loss : 0.6642354787553668\n",
      "Epoch:164,Loss : 0.6642193329766509\n",
      "Epoch:165,Loss : 0.664203533407416\n",
      "Epoch:166,Loss : 0.6641880703483065\n",
      "Epoch:167,Loss : 0.6641729343992957\n",
      "Epoch:168,Loss : 0.6641581164499206\n",
      "Epoch:169,Loss : 0.6641436076698503\n",
      "Epoch:170,Loss : 0.6641293994997729\n",
      "Epoch:171,Loss : 0.6641154836425939\n",
      "Epoch:172,Loss : 0.6641018520549308\n",
      "Epoch:173,Loss : 0.6640884969388978\n",
      "Epoch:174,Loss : 0.6640754107341674\n",
      "Epoch:175,Loss : 0.6640625861102997\n",
      "Epoch:176,Loss : 0.6640500159593308\n",
      "Epoch:177,Loss : 0.6640376933886117\n",
      "Epoch:178,Loss : 0.6640256117138869\n",
      "Epoch:179,Loss : 0.6640137644526056\n",
      "Epoch:180,Loss : 0.6640021453174586\n",
      "Epoch:181,Loss : 0.6639907482101309\n",
      "Epoch:182,Loss : 0.6639795672152635\n",
      "Epoch:183,Loss : 0.663968596594617\n",
      "Epoch:184,Loss : 0.66395783078143\n",
      "Epoch:185,Loss : 0.6639472643749661\n",
      "Epoch:186,Loss : 0.6639368921352427\n",
      "Epoch:187,Loss : 0.6639267089779324\n",
      "Epoch:188,Loss : 0.6639167099694383\n",
      "Epoch:189,Loss : 0.6639068903221288\n",
      "Epoch:190,Loss : 0.6638972453897317\n",
      "Epoch:191,Loss : 0.6638877706628808\n",
      "Epoch:192,Loss : 0.6638784617648092\n",
      "Epoch:193,Loss : 0.6638693144471846\n",
      "Epoch:194,Loss : 0.6638603245860824\n",
      "Epoch:195,Loss : 0.6638514881780901\n",
      "Epoch:196,Loss : 0.6638428013365416\n",
      "Epoch:197,Loss : 0.6638342602878724\n",
      "Epoch:198,Loss : 0.6638258613680971\n",
      "Epoch:199,Loss : 0.6638176010193998\n",
      "Epoch:200,Loss : 0.6638094757868368\n",
      "Epoch:201,Loss : 0.6638014823151479\n",
      "Epoch:202,Loss : 0.6637936173456689\n",
      "Epoch:203,Loss : 0.6637858777133482\n",
      "Epoch:204,Loss : 0.6637782603438561\n",
      "Epoch:205,Loss : 0.6637707622507927\n",
      "Epoch:206,Loss : 0.6637633805329812\n",
      "Epoch:207,Loss : 0.6637561123718531\n",
      "Epoch:208,Loss : 0.6637489550289146\n",
      "Epoch:209,Loss : 0.6637419058432975\n",
      "Epoch:210,Loss : 0.6637349622293867\n",
      "Epoch:211,Loss : 0.6637281216745246\n",
      "Epoch:212,Loss : 0.6637213817367908\n",
      "Epoch:213,Loss : 0.6637147400428517\n",
      "Epoch:214,Loss : 0.6637081942858768\n",
      "Epoch:215,Loss : 0.6637017422235271\n",
      "Epoch:216,Loss : 0.6636953816760021\n",
      "Epoch:217,Loss : 0.6636891105241529\n",
      "Epoch:218,Loss : 0.663682926707653\n",
      "Epoch:219,Loss : 0.6636768282232292\n",
      "Epoch:220,Loss : 0.6636708131229463\n",
      "Epoch:221,Loss : 0.663664879512549\n",
      "Epoch:222,Loss : 0.6636590255498546\n",
      "Epoch:223,Loss : 0.6636532494431951\n",
      "Epoch:224,Loss : 0.6636475494499131\n",
      "Epoch:225,Loss : 0.6636419238748987\n",
      "Epoch:226,Loss : 0.6636363710691787\n",
      "Epoch:227,Loss : 0.6636308894285452\n",
      "Epoch:228,Loss : 0.6636254773922311\n",
      "Epoch:229,Loss : 0.6636201334416232\n",
      "Epoch:230,Loss : 0.6636148560990197\n",
      "Epoch:231,Loss : 0.6636096439264231\n",
      "Epoch:232,Loss : 0.6636044955243721\n",
      "Epoch:233,Loss : 0.6635994095308106\n",
      "Epoch:234,Loss : 0.6635943846199907\n",
      "Epoch:235,Loss : 0.6635894195014096\n",
      "Epoch:236,Loss : 0.6635845129187811\n",
      "Epoch:237,Loss : 0.6635796636490369\n",
      "Epoch:238,Loss : 0.6635748705013603\n",
      "Epoch:239,Loss : 0.6635701323162487\n",
      "Epoch:240,Loss : 0.6635654479646057\n",
      "Epoch:241,Loss : 0.6635608163468603\n",
      "Epoch:242,Loss : 0.6635562363921147\n",
      "Epoch:243,Loss : 0.6635517070573163\n",
      "Epoch:244,Loss : 0.663547227326456\n",
      "Epoch:245,Loss : 0.6635427962097921\n",
      "Epoch:246,Loss : 0.6635384127430951\n",
      "Epoch:247,Loss : 0.6635340759869182\n",
      "Epoch:248,Loss : 0.6635297850258893\n",
      "Epoch:249,Loss : 0.6635255389680236\n",
      "Epoch:250,Loss : 0.6635213369440573\n",
      "Epoch:251,Loss : 0.6635171781068034\n",
      "Epoch:252,Loss : 0.6635130616305241\n",
      "Epoch:253,Loss : 0.6635089867103234\n",
      "Epoch:254,Loss : 0.6635049525615591\n",
      "Epoch:255,Loss : 0.6635009584192713\n",
      "Epoch:256,Loss : 0.6634970035376274\n",
      "Epoch:257,Loss : 0.6634930871893859\n",
      "Epoch:258,Loss : 0.6634892086653745\n",
      "Epoch:259,Loss : 0.6634853672739841\n",
      "Epoch:260,Loss : 0.6634815623406787\n",
      "Epoch:261,Loss : 0.6634777932075191\n",
      "Epoch:262,Loss : 0.6634740592327011\n",
      "Epoch:263,Loss : 0.6634703597901073\n",
      "Epoch:264,Loss : 0.6634666942688734\n",
      "Epoch:265,Loss : 0.6634630620729646\n",
      "Epoch:266,Loss : 0.6634594626207672\n",
      "Epoch:267,Loss : 0.6634558953446915\n",
      "Epoch:268,Loss : 0.6634523596907862\n",
      "Epoch:269,Loss : 0.6634488551183634\n",
      "Epoch:270,Loss : 0.6634453810996361\n",
      "Epoch:271,Loss : 0.6634419371193657\n",
      "Epoch:272,Loss : 0.6634385226745196\n",
      "Epoch:273,Loss : 0.6634351372739392\n",
      "Epoch:274,Loss : 0.6634317804380172\n",
      "Epoch:275,Loss : 0.6634284516983846\n",
      "Epoch:276,Loss : 0.6634251505976074\n",
      "Epoch:277,Loss : 0.6634218766888907\n",
      "Epoch:278,Loss : 0.6634186295357926\n",
      "Epoch:279,Loss : 0.6634154087119458\n",
      "Epoch:280,Loss : 0.6634122138007879\n",
      "Epoch:281,Loss : 0.6634090443952987\n",
      "Epoch:282,Loss : 0.6634059000977454\n",
      "Epoch:283,Loss : 0.6634027805194356\n",
      "Epoch:284,Loss : 0.6633996852804765\n",
      "Epoch:285,Loss : 0.6633966140095422\n",
      "Epoch:286,Loss : 0.6633935663436463\n",
      "Epoch:287,Loss : 0.6633905419279221\n",
      "Epoch:288,Loss : 0.6633875404154084\n",
      "Epoch:289,Loss : 0.6633845614668424\n",
      "Epoch:290,Loss : 0.6633816047504566\n",
      "Epoch:291,Loss : 0.6633786699417844\n",
      "Epoch:292,Loss : 0.6633757567234676\n",
      "Epoch:293,Loss : 0.663372864785073\n",
      "Epoch:294,Loss : 0.6633699938229112\n",
      "Epoch:295,Loss : 0.6633671435398626\n",
      "Epoch:296,Loss : 0.6633643136452068\n",
      "Epoch:297,Loss : 0.6633615038544586\n",
      "Epoch:298,Loss : 0.663358713889206\n",
      "Epoch:299,Loss : 0.6633559434769558\n",
      "Epoch:300,Loss : 0.663353192350981\n",
      "Epoch:301,Loss : 0.6633504602501737\n",
      "Epoch:302,Loss : 0.6633477469189026\n",
      "Epoch:303,Loss : 0.663345052106872\n",
      "Epoch:304,Loss : 0.6633423755689888\n",
      "Epoch:305,Loss : 0.6633397170652288\n",
      "Epoch:306,Loss : 0.6633370763605103\n",
      "Epoch:307,Loss : 0.6633344532245689\n",
      "Epoch:308,Loss : 0.6633318474318367\n",
      "Epoch:309,Loss : 0.6633292587613245\n",
      "Epoch:310,Loss : 0.6633266869965089\n",
      "Epoch:311,Loss : 0.6633241319252179\n",
      "Epoch:312,Loss : 0.6633215933395263\n",
      "Epoch:313,Loss : 0.6633190710356488\n",
      "Epoch:314,Loss : 0.6633165648138368\n",
      "Epoch:315,Loss : 0.663314074478281\n",
      "Epoch:316,Loss : 0.6633115998370132\n",
      "Epoch:317,Loss : 0.6633091407018117\n",
      "Epoch:318,Loss : 0.6633066968881113\n",
      "Epoch:319,Loss : 0.6633042682149132\n",
      "Epoch:320,Loss : 0.6633018545046978\n",
      "Epoch:321,Loss : 0.6632994555833418\n",
      "Epoch:322,Loss : 0.6632970712800346\n",
      "Epoch:323,Loss : 0.6632947014271992\n",
      "Epoch:324,Loss : 0.6632923458604146\n",
      "Epoch:325,Loss : 0.6632900044183395\n",
      "Epoch:326,Loss : 0.6632876769426396\n",
      "Epoch:327,Loss : 0.6632853632779151\n",
      "Epoch:328,Loss : 0.6632830632716314\n",
      "Epoch:329,Loss : 0.6632807767740512\n",
      "Epoch:330,Loss : 0.6632785036381684\n",
      "Epoch:331,Loss : 0.6632762437196438\n",
      "Epoch:332,Loss : 0.6632739968767424\n",
      "Epoch:333,Loss : 0.6632717629702727\n",
      "Epoch:334,Loss : 0.6632695418635268\n",
      "Epoch:335,Loss : 0.6632673334222233\n",
      "Epoch:336,Loss : 0.6632651375144507\n",
      "Epoch:337,Loss : 0.6632629540106125\n",
      "Epoch:338,Loss : 0.6632607827833741\n",
      "Epoch:339,Loss : 0.6632586237076109\n",
      "Epoch:340,Loss : 0.6632564766603573\n",
      "Epoch:341,Loss : 0.6632543415207576\n",
      "Epoch:342,Loss : 0.6632522181700181\n",
      "Epoch:343,Loss : 0.6632501064913606\n",
      "Epoch:344,Loss : 0.6632480063699766\n",
      "Epoch:345,Loss : 0.6632459176929821\n",
      "Epoch:346,Loss : 0.6632438403493762\n",
      "Epoch:347,Loss : 0.6632417742299973\n",
      "Epoch:348,Loss : 0.6632397192274824\n",
      "Epoch:349,Loss : 0.663237675236229\n",
      "Epoch:350,Loss : 0.6632356421523526\n",
      "Epoch:351,Loss : 0.6632336198736521\n",
      "Epoch:352,Loss : 0.6632316082995706\n",
      "Epoch:353,Loss : 0.6632296073311609\n",
      "Epoch:354,Loss : 0.6632276168710488\n",
      "Epoch:355,Loss : 0.6632256368234006\n",
      "Epoch:356,Loss : 0.6632236670938879\n",
      "Epoch:357,Loss : 0.6632217075896567\n",
      "Epoch:358,Loss : 0.663219758219294\n",
      "Epoch:359,Loss : 0.6632178188927987\n",
      "Epoch:360,Loss : 0.66321588952155\n",
      "Epoch:361,Loss : 0.6632139700182784\n",
      "Epoch:362,Loss : 0.6632120602970374\n",
      "Epoch:363,Loss : 0.6632101602731754\n",
      "Epoch:364,Loss : 0.6632082698633076\n",
      "Epoch:365,Loss : 0.6632063889852914\n",
      "Epoch:366,Loss : 0.6632045175581973\n",
      "Epoch:367,Loss : 0.6632026555022873\n",
      "Epoch:368,Loss : 0.6632008027389872\n",
      "Epoch:369,Loss : 0.6631989591908645\n",
      "Epoch:370,Loss : 0.6631971247816036\n",
      "Epoch:371,Loss : 0.6631952994359837\n",
      "Epoch:372,Loss : 0.6631934830798559\n",
      "Epoch:373,Loss : 0.6631916756401224\n",
      "Epoch:374,Loss : 0.6631898770447134\n",
      "Epoch:375,Loss : 0.6631880872225685\n",
      "Epoch:376,Loss : 0.6631863061036147\n",
      "Epoch:377,Loss : 0.6631845336187477\n",
      "Epoch:378,Loss : 0.6631827696998119\n",
      "Epoch:379,Loss : 0.6631810142795819\n",
      "Epoch:380,Loss : 0.6631792672917443\n",
      "Epoch:381,Loss : 0.6631775286708792\n",
      "Epoch:382,Loss : 0.6631757983524433\n",
      "Epoch:383,Loss : 0.6631740762727523\n",
      "Epoch:384,Loss : 0.6631723623689645\n",
      "Epoch:385,Loss : 0.6631706565790642\n",
      "Epoch:386,Loss : 0.663168958841846\n",
      "Epoch:387,Loss : 0.6631672690968994\n",
      "Epoch:388,Loss : 0.663165587284593\n",
      "Epoch:389,Loss : 0.6631639133460606\n",
      "Epoch:390,Loss : 0.6631622472231855\n",
      "Epoch:391,Loss : 0.6631605888585878\n",
      "Epoch:392,Loss : 0.6631589381956089\n",
      "Epoch:393,Loss : 0.6631572951782998\n",
      "Epoch:394,Loss : 0.6631556597514066\n",
      "Epoch:395,Loss : 0.6631540318603577\n",
      "Epoch:396,Loss : 0.663152411451252\n",
      "Epoch:397,Loss : 0.6631507984708459\n",
      "Epoch:398,Loss : 0.6631491928665412\n",
      "Epoch:399,Loss : 0.663147594586374\n",
      "Epoch:400,Loss : 0.6631460035790028\n",
      "Epoch:401,Loss : 0.6631444197936973\n",
      "Epoch:402,Loss : 0.663142843180327\n",
      "Epoch:403,Loss : 0.6631412736893516\n",
      "Epoch:404,Loss : 0.6631397112718092\n",
      "Epoch:405,Loss : 0.6631381558793068\n",
      "Epoch:406,Loss : 0.6631366074640105\n",
      "Epoch:407,Loss : 0.6631350659786344\n",
      "Epoch:408,Loss : 0.6631335313764332\n",
      "Epoch:409,Loss : 0.6631320036111903\n",
      "Epoch:410,Loss : 0.6631304826372111\n",
      "Epoch:411,Loss : 0.6631289684093113\n",
      "Epoch:412,Loss : 0.6631274608828115\n",
      "Epoch:413,Loss : 0.6631259600135253\n",
      "Epoch:414,Loss : 0.6631244657577527\n",
      "Epoch:415,Loss : 0.663122978072272\n",
      "Epoch:416,Loss : 0.6631214969143308\n",
      "Epoch:417,Loss : 0.6631200222416391\n",
      "Epoch:418,Loss : 0.6631185540123611\n",
      "Epoch:419,Loss : 0.663117092185107\n",
      "Epoch:420,Loss : 0.6631156367189276\n",
      "Epoch:421,Loss : 0.663114187573305\n",
      "Epoch:422,Loss : 0.6631127447081467\n",
      "Epoch:423,Loss : 0.6631113080837784\n",
      "Epoch:424,Loss : 0.6631098776609372\n",
      "Epoch:425,Loss : 0.6631084534007649\n",
      "Epoch:426,Loss : 0.6631070352648019\n",
      "Epoch:427,Loss : 0.6631056232149803\n",
      "Epoch:428,Loss : 0.6631042172136183\n",
      "Epoch:429,Loss : 0.6631028172234134\n",
      "Epoch:430,Loss : 0.6631014232074373\n",
      "Epoch:431,Loss : 0.6631000351291289\n",
      "Epoch:432,Loss : 0.6630986529522904\n",
      "Epoch:433,Loss : 0.6630972766410795\n",
      "Epoch:434,Loss : 0.6630959061600054\n",
      "Epoch:435,Loss : 0.6630945414739229\n",
      "Epoch:436,Loss : 0.6630931825480274\n",
      "Epoch:437,Loss : 0.6630918293478499\n",
      "Epoch:438,Loss : 0.663090481839251\n",
      "Epoch:439,Loss : 0.6630891399884169\n",
      "Epoch:440,Loss : 0.6630878037618545\n",
      "Epoch:441,Loss : 0.6630864731263857\n",
      "Epoch:442,Loss : 0.6630851480491441\n",
      "Epoch:443,Loss : 0.663083828497569\n",
      "Epoch:444,Loss : 0.663082514439403\n",
      "Epoch:445,Loss : 0.6630812058426845\n",
      "Epoch:446,Loss : 0.6630799026757466\n",
      "Epoch:447,Loss : 0.6630786049072104\n",
      "Epoch:448,Loss : 0.6630773125059827\n",
      "Epoch:449,Loss : 0.6630760254412508\n",
      "Epoch:450,Loss : 0.663074743682479\n",
      "Epoch:451,Loss : 0.6630734671994037\n",
      "Epoch:452,Loss : 0.6630721959620318\n",
      "Epoch:453,Loss : 0.6630709299406349\n",
      "Epoch:454,Loss : 0.663069669105746\n",
      "Epoch:455,Loss : 0.6630684134281563\n",
      "Epoch:456,Loss : 0.6630671628789115\n",
      "Epoch:457,Loss : 0.6630659174293091\n",
      "Epoch:458,Loss : 0.663064677050893\n",
      "Epoch:459,Loss : 0.6630634417154518\n",
      "Epoch:460,Loss : 0.6630622113950156\n",
      "Epoch:461,Loss : 0.6630609860618518\n",
      "Epoch:462,Loss : 0.6630597656884617\n",
      "Epoch:463,Loss : 0.6630585502475792\n",
      "Epoch:464,Loss : 0.6630573397121663\n",
      "Epoch:465,Loss : 0.6630561340554099\n",
      "Epoch:466,Loss : 0.6630549332507196\n",
      "Epoch:467,Loss : 0.6630537372717249\n",
      "Epoch:468,Loss : 0.6630525460922718\n",
      "Epoch:469,Loss : 0.66305135968642\n",
      "Epoch:470,Loss : 0.6630501780284408\n",
      "Epoch:471,Loss : 0.6630490010928137\n",
      "Epoch:472,Loss : 0.6630478288542242\n",
      "Epoch:473,Loss : 0.6630466612875611\n",
      "Epoch:474,Loss : 0.663045498367914\n",
      "Epoch:475,Loss : 0.6630443400705708\n",
      "Epoch:476,Loss : 0.6630431863710143\n",
      "Epoch:477,Loss : 0.6630420372449222\n",
      "Epoch:478,Loss : 0.6630408926681617\n",
      "Epoch:479,Loss : 0.6630397526167897\n",
      "Epoch:480,Loss : 0.6630386170670486\n",
      "Epoch:481,Loss : 0.6630374859953655\n",
      "Epoch:482,Loss : 0.6630363593783494\n",
      "Epoch:483,Loss : 0.6630352371927885\n",
      "Epoch:484,Loss : 0.6630341194156484\n",
      "Epoch:485,Loss : 0.6630330060240712\n",
      "Epoch:486,Loss : 0.6630318969953718\n",
      "Epoch:487,Loss : 0.663030792307036\n",
      "Epoch:488,Loss : 0.6630296919367192\n",
      "Epoch:489,Loss : 0.6630285958622448\n",
      "Epoch:490,Loss : 0.6630275040616007\n",
      "Epoch:491,Loss : 0.663026416512939\n",
      "Epoch:492,Loss : 0.663025333194573\n",
      "Epoch:493,Loss : 0.6630242540849762\n",
      "Epoch:494,Loss : 0.6630231791627801\n",
      "Epoch:495,Loss : 0.6630221084067719\n",
      "Epoch:496,Loss : 0.6630210417958938\n",
      "Epoch:497,Loss : 0.6630199793092405\n",
      "Epoch:498,Loss : 0.663018920926058\n",
      "Epoch:499,Loss : 0.6630178666257414\n",
      "Epoch:500,Loss : 0.6630168163878333\n",
      "Epoch:501,Loss : 0.6630157701920232\n",
      "Epoch:502,Loss : 0.6630147280181443\n",
      "Epoch:503,Loss : 0.6630136898461734\n",
      "Epoch:504,Loss : 0.663012655656228\n",
      "Epoch:505,Loss : 0.6630116254285664\n",
      "Epoch:506,Loss : 0.663010599143584\n",
      "Epoch:507,Loss : 0.6630095767818147\n",
      "Epoch:508,Loss : 0.6630085583239265\n",
      "Epoch:509,Loss : 0.663007543750722\n",
      "Epoch:510,Loss : 0.6630065330431367\n",
      "Epoch:511,Loss : 0.6630055261822368\n",
      "Epoch:512,Loss : 0.6630045231492188\n",
      "Epoch:513,Loss : 0.6630035239254074\n",
      "Epoch:514,Loss : 0.6630025284922546\n",
      "Epoch:515,Loss : 0.6630015368313386\n",
      "Epoch:516,Loss : 0.6630005489243618\n",
      "Epoch:517,Loss : 0.6629995647531503\n",
      "Epoch:518,Loss : 0.662998584299652\n",
      "Epoch:519,Loss : 0.6629976075459358\n",
      "Epoch:520,Loss : 0.6629966344741907\n",
      "Epoch:521,Loss : 0.6629956650667234\n",
      "Epoch:522,Loss : 0.6629946993059583\n",
      "Epoch:523,Loss : 0.662993737174436\n",
      "Epoch:524,Loss : 0.6629927786548121\n",
      "Epoch:525,Loss : 0.6629918237298558\n",
      "Epoch:526,Loss : 0.6629908723824495\n",
      "Epoch:527,Loss : 0.6629899245955867\n",
      "Epoch:528,Loss : 0.662988980352372\n",
      "Epoch:529,Loss : 0.6629880396360193\n",
      "Epoch:530,Loss : 0.6629871024298507\n",
      "Epoch:531,Loss : 0.6629861687172964\n",
      "Epoch:532,Loss : 0.6629852384818926\n",
      "Epoch:533,Loss : 0.6629843117072808\n",
      "Epoch:534,Loss : 0.6629833883772073\n",
      "Epoch:535,Loss : 0.6629824684755214\n",
      "Epoch:536,Loss : 0.6629815519861753\n",
      "Epoch:537,Loss : 0.6629806388932222\n",
      "Epoch:538,Loss : 0.6629797291808168\n",
      "Epoch:539,Loss : 0.6629788228332121\n",
      "Epoch:540,Loss : 0.6629779198347613\n",
      "Epoch:541,Loss : 0.6629770201699143\n",
      "Epoch:542,Loss : 0.6629761238232185\n",
      "Epoch:543,Loss : 0.6629752307793175\n",
      "Epoch:544,Loss : 0.6629743410229496\n",
      "Epoch:545,Loss : 0.6629734545389478\n",
      "Epoch:546,Loss : 0.6629725713122387\n",
      "Epoch:547,Loss : 0.6629716913278414\n",
      "Epoch:548,Loss : 0.6629708145708667\n",
      "Epoch:549,Loss : 0.6629699410265166\n",
      "Epoch:550,Loss : 0.6629690706800835\n",
      "Epoch:551,Loss : 0.662968203516949\n",
      "Epoch:552,Loss : 0.6629673395225835\n",
      "Epoch:553,Loss : 0.6629664786825452\n",
      "Epoch:554,Loss : 0.6629656209824792\n",
      "Epoch:555,Loss : 0.662964766408118\n",
      "Epoch:556,Loss : 0.6629639149452784\n",
      "Epoch:557,Loss : 0.6629630665798627\n",
      "Epoch:558,Loss : 0.662962221297858\n",
      "Epoch:559,Loss : 0.6629613790853333\n",
      "Epoch:560,Loss : 0.6629605399284421\n",
      "Epoch:561,Loss : 0.6629597038134188\n",
      "Epoch:562,Loss : 0.6629588707265796\n",
      "Epoch:563,Loss : 0.6629580406543214\n",
      "Epoch:564,Loss : 0.662957213583121\n",
      "Epoch:565,Loss : 0.6629563894995345\n",
      "Epoch:566,Loss : 0.6629555683901971\n",
      "Epoch:567,Loss : 0.6629547502418218\n",
      "Epoch:568,Loss : 0.6629539350411985\n",
      "Epoch:569,Loss : 0.6629531227751947\n",
      "Epoch:570,Loss : 0.6629523134307536\n",
      "Epoch:571,Loss : 0.662951506994894\n",
      "Epoch:572,Loss : 0.6629507034547096\n",
      "Epoch:573,Loss : 0.6629499027973688\n",
      "Epoch:574,Loss : 0.6629491050101131\n",
      "Epoch:575,Loss : 0.6629483100802572\n",
      "Epoch:576,Loss : 0.6629475179951891\n",
      "Epoch:577,Loss : 0.6629467287423676\n",
      "Epoch:578,Loss : 0.6629459423093241\n",
      "Epoch:579,Loss : 0.66294515868366\n",
      "Epoch:580,Loss : 0.662944377853047\n",
      "Epoch:581,Loss : 0.6629435998052272\n",
      "Epoch:582,Loss : 0.6629428245280112\n",
      "Epoch:583,Loss : 0.6629420520092784\n",
      "Epoch:584,Loss : 0.6629412822369767\n",
      "Epoch:585,Loss : 0.6629405151991211\n",
      "Epoch:586,Loss : 0.6629397508837936\n",
      "Epoch:587,Loss : 0.6629389892791436\n",
      "Epoch:588,Loss : 0.6629382303733856\n",
      "Epoch:589,Loss : 0.6629374741547999\n",
      "Epoch:590,Loss : 0.6629367206117319\n",
      "Epoch:591,Loss : 0.6629359697325918\n",
      "Epoch:592,Loss : 0.6629352215058534\n",
      "Epoch:593,Loss : 0.6629344759200543\n",
      "Epoch:594,Loss : 0.6629337329637951\n",
      "Epoch:595,Loss : 0.6629329926257393\n",
      "Epoch:596,Loss : 0.6629322548946124\n",
      "Epoch:597,Loss : 0.6629315197592013\n",
      "Epoch:598,Loss : 0.6629307872083545\n",
      "Epoch:599,Loss : 0.6629300572309814\n",
      "Epoch:600,Loss : 0.6629293298160512\n",
      "Epoch:601,Loss : 0.6629286049525936\n",
      "Epoch:602,Loss : 0.6629278826296973\n",
      "Epoch:603,Loss : 0.66292716283651\n",
      "Epoch:604,Loss : 0.6629264455622386\n",
      "Epoch:605,Loss : 0.6629257307961475\n",
      "Epoch:606,Loss : 0.6629250185275591\n",
      "Epoch:607,Loss : 0.6629243087458532\n",
      "Epoch:608,Loss : 0.6629236014404665\n",
      "Epoch:609,Loss : 0.6629228966008921\n",
      "Epoch:610,Loss : 0.6629221942166793\n",
      "Epoch:611,Loss : 0.6629214942774332\n",
      "Epoch:612,Loss : 0.6629207967728136\n",
      "Epoch:613,Loss : 0.6629201016925362\n",
      "Epoch:614,Loss : 0.6629194090263705\n",
      "Epoch:615,Loss : 0.6629187187641405\n",
      "Epoch:616,Loss : 0.662918030895724\n",
      "Epoch:617,Loss : 0.6629173454110517\n",
      "Epoch:618,Loss : 0.6629166623001077\n",
      "Epoch:619,Loss : 0.6629159815529289\n",
      "Epoch:620,Loss : 0.6629153031596045\n",
      "Epoch:621,Loss : 0.6629146271102748\n",
      "Epoch:622,Loss : 0.6629139533951327\n",
      "Epoch:623,Loss : 0.6629132820044216\n",
      "Epoch:624,Loss : 0.6629126129284363\n",
      "Epoch:625,Loss : 0.6629119461575218\n",
      "Epoch:626,Loss : 0.6629112816820733\n",
      "Epoch:627,Loss : 0.6629106194925355\n",
      "Epoch:628,Loss : 0.6629099595794031\n",
      "Epoch:629,Loss : 0.6629093019332201\n",
      "Epoch:630,Loss : 0.6629086465445784\n",
      "Epoch:631,Loss : 0.6629079934041192\n",
      "Epoch:632,Loss : 0.6629073425025316\n",
      "Epoch:633,Loss : 0.6629066938305523\n",
      "Epoch:634,Loss : 0.6629060473789661\n",
      "Epoch:635,Loss : 0.6629054031386045\n",
      "Epoch:636,Loss : 0.6629047611003457\n",
      "Epoch:637,Loss : 0.6629041212551152\n",
      "Epoch:638,Loss : 0.6629034835938842\n",
      "Epoch:639,Loss : 0.6629028481076699\n",
      "Epoch:640,Loss : 0.6629022147875354\n",
      "Epoch:641,Loss : 0.6629015836245891\n",
      "Epoch:642,Loss : 0.6629009546099839\n",
      "Epoch:643,Loss : 0.6629003277349186\n",
      "Epoch:644,Loss : 0.6628997029906352\n",
      "Epoch:645,Loss : 0.6628990803684206\n",
      "Epoch:646,Loss : 0.6628984598596056\n",
      "Epoch:647,Loss : 0.6628978414555646\n",
      "Epoch:648,Loss : 0.6628972251477151\n",
      "Epoch:649,Loss : 0.6628966109275175\n",
      "Epoch:650,Loss : 0.6628959987864754\n",
      "Epoch:651,Loss : 0.6628953887161345\n",
      "Epoch:652,Loss : 0.6628947807080832\n",
      "Epoch:653,Loss : 0.6628941747539514\n",
      "Epoch:654,Loss : 0.6628935708454108\n",
      "Epoch:655,Loss : 0.6628929689741748\n",
      "Epoch:656,Loss : 0.6628923691319976\n",
      "Epoch:657,Loss : 0.6628917713106741\n",
      "Epoch:658,Loss : 0.6628911755020405\n",
      "Epoch:659,Loss : 0.662890581697973\n",
      "Epoch:660,Loss : 0.662889989890388\n",
      "Epoch:661,Loss : 0.6628894000712415\n",
      "Epoch:662,Loss : 0.6628888122325297\n",
      "Epoch:663,Loss : 0.6628882263662873\n",
      "Epoch:664,Loss : 0.6628876424645889\n",
      "Epoch:665,Loss : 0.662887060519548\n",
      "Epoch:666,Loss : 0.6628864805233162\n",
      "Epoch:667,Loss : 0.6628859024680837\n",
      "Epoch:668,Loss : 0.6628853263460791\n",
      "Epoch:669,Loss : 0.6628847521495687\n",
      "Epoch:670,Loss : 0.6628841798708565\n",
      "Epoch:671,Loss : 0.6628836095022844\n",
      "Epoch:672,Loss : 0.6628830410362307\n",
      "Epoch:673,Loss : 0.6628824744651115\n",
      "Epoch:674,Loss : 0.6628819097813792\n",
      "Epoch:675,Loss : 0.662881346977523\n",
      "Epoch:676,Loss : 0.6628807860460683\n",
      "Epoch:677,Loss : 0.6628802269795764\n",
      "Epoch:678,Loss : 0.662879669770645\n",
      "Epoch:679,Loss : 0.6628791144119075\n",
      "Epoch:680,Loss : 0.6628785608960319\n",
      "Epoch:681,Loss : 0.6628780092157225\n",
      "Epoch:682,Loss : 0.6628774593637177\n",
      "Epoch:683,Loss : 0.6628769113327917\n",
      "Epoch:684,Loss : 0.6628763651157527\n",
      "Epoch:685,Loss : 0.662875820705443\n",
      "Epoch:686,Loss : 0.6628752780947398\n",
      "Epoch:687,Loss : 0.6628747372765541\n",
      "Epoch:688,Loss : 0.6628741982438303\n",
      "Epoch:689,Loss : 0.662873660989547\n",
      "Epoch:690,Loss : 0.6628731255067157\n",
      "Epoch:691,Loss : 0.6628725917883814\n",
      "Epoch:692,Loss : 0.6628720598276219\n",
      "Epoch:693,Loss : 0.6628715296175479\n",
      "Epoch:694,Loss : 0.6628710011513028\n",
      "Epoch:695,Loss : 0.6628704744220624\n",
      "Epoch:696,Loss : 0.6628699494230343\n",
      "Epoch:697,Loss : 0.6628694261474587\n",
      "Epoch:698,Loss : 0.6628689045886078\n",
      "Epoch:699,Loss : 0.6628683847397847\n",
      "Epoch:700,Loss : 0.6628678665943245\n",
      "Epoch:701,Loss : 0.6628673501455937\n",
      "Epoch:702,Loss : 0.6628668353869898\n",
      "Epoch:703,Loss : 0.6628663223119406\n",
      "Epoch:704,Loss : 0.6628658109139057\n",
      "Epoch:705,Loss : 0.6628653011863751\n",
      "Epoch:706,Loss : 0.6628647931228683\n",
      "Epoch:707,Loss : 0.662864286716936\n",
      "Epoch:708,Loss : 0.6628637819621587\n",
      "Epoch:709,Loss : 0.6628632788521462\n",
      "Epoch:710,Loss : 0.6628627773805393\n",
      "Epoch:711,Loss : 0.6628622775410071\n",
      "Epoch:712,Loss : 0.6628617793272485\n",
      "Epoch:713,Loss : 0.6628612827329922\n",
      "Epoch:714,Loss : 0.6628607877519945\n",
      "Epoch:715,Loss : 0.6628602943780422\n",
      "Epoch:716,Loss : 0.6628598026049494\n",
      "Epoch:717,Loss : 0.6628593124265603\n",
      "Epoch:718,Loss : 0.6628588238367458\n",
      "Epoch:719,Loss : 0.6628583368294062\n",
      "Epoch:720,Loss : 0.6628578513984699\n",
      "Epoch:721,Loss : 0.6628573675378922\n",
      "Epoch:722,Loss : 0.6628568852416569\n",
      "Epoch:723,Loss : 0.6628564045037756\n",
      "Epoch:724,Loss : 0.6628559253182867\n",
      "Epoch:725,Loss : 0.6628554476792564\n",
      "Epoch:726,Loss : 0.662854971580778\n",
      "Epoch:727,Loss : 0.6628544970169713\n",
      "Epoch:728,Loss : 0.6628540239819835\n",
      "Epoch:729,Loss : 0.6628535524699881\n",
      "Epoch:730,Loss : 0.6628530824751854\n",
      "Epoch:731,Loss : 0.662852613991802\n",
      "Epoch:732,Loss : 0.6628521470140907\n",
      "Epoch:733,Loss : 0.6628516815363306\n",
      "Epoch:734,Loss : 0.6628512175528263\n",
      "Epoch:735,Loss : 0.6628507550579087\n",
      "Epoch:736,Loss : 0.6628502940459342\n",
      "Epoch:737,Loss : 0.6628498345112845\n",
      "Epoch:738,Loss : 0.6628493764483672\n",
      "Epoch:739,Loss : 0.6628489198516145\n",
      "Epoch:740,Loss : 0.6628484647154846\n",
      "Epoch:741,Loss : 0.6628480110344595\n",
      "Epoch:742,Loss : 0.662847558803047\n",
      "Epoch:743,Loss : 0.6628471080157793\n",
      "Epoch:744,Loss : 0.6628466586672129\n",
      "Epoch:745,Loss : 0.6628462107519293\n",
      "Epoch:746,Loss : 0.6628457642645339\n",
      "Epoch:747,Loss : 0.662845319199656\n",
      "Epoch:748,Loss : 0.6628448755519496\n",
      "Epoch:749,Loss : 0.6628444333160926\n",
      "Epoch:750,Loss : 0.6628439924867856\n",
      "Epoch:751,Loss : 0.6628435530587542\n",
      "Epoch:752,Loss : 0.6628431150267468\n",
      "Epoch:753,Loss : 0.6628426783855352\n",
      "Epoch:754,Loss : 0.6628422431299149\n",
      "Epoch:755,Loss : 0.6628418092547043\n",
      "Epoch:756,Loss : 0.6628413767547443\n",
      "Epoch:757,Loss : 0.6628409456248996\n",
      "Epoch:758,Loss : 0.6628405158600572\n",
      "Epoch:759,Loss : 0.6628400874551269\n",
      "Epoch:760,Loss : 0.6628396604050407\n",
      "Epoch:761,Loss : 0.6628392347047535\n",
      "Epoch:762,Loss : 0.6628388103492422\n",
      "Epoch:763,Loss : 0.6628383873335058\n",
      "Epoch:764,Loss : 0.6628379656525657\n",
      "Epoch:765,Loss : 0.6628375453014652\n",
      "Epoch:766,Loss : 0.6628371262752687\n",
      "Epoch:767,Loss : 0.6628367085690634\n",
      "Epoch:768,Loss : 0.6628362921779576\n",
      "Epoch:769,Loss : 0.6628358770970805\n",
      "Epoch:770,Loss : 0.6628354633215837\n",
      "Epoch:771,Loss : 0.6628350508466395\n",
      "Epoch:772,Loss : 0.6628346396674417\n",
      "Epoch:773,Loss : 0.6628342297792043\n",
      "Epoch:774,Loss : 0.6628338211771632\n",
      "Epoch:775,Loss : 0.6628334138565743\n",
      "Epoch:776,Loss : 0.6628330078127153\n",
      "Epoch:777,Loss : 0.6628326030408831\n",
      "Epoch:778,Loss : 0.6628321995363964\n",
      "Epoch:779,Loss : 0.6628317972945931\n",
      "Epoch:780,Loss : 0.6628313963108321\n",
      "Epoch:781,Loss : 0.6628309965804926\n",
      "Epoch:782,Loss : 0.6628305980989736\n",
      "Epoch:783,Loss : 0.6628302008616939\n",
      "Epoch:784,Loss : 0.662829804864092\n",
      "Epoch:785,Loss : 0.6628294101016272\n",
      "Epoch:786,Loss : 0.6628290165697771\n",
      "Epoch:787,Loss : 0.6628286242640398\n",
      "Epoch:788,Loss : 0.6628282331799322\n",
      "Epoch:789,Loss : 0.6628278433129914\n",
      "Epoch:790,Loss : 0.6628274546587728\n",
      "Epoch:791,Loss : 0.6628270672128516\n",
      "Epoch:792,Loss : 0.6628266809708216\n",
      "Epoch:793,Loss : 0.6628262959282961\n",
      "Epoch:794,Loss : 0.6628259120809069\n",
      "Epoch:795,Loss : 0.6628255294243046\n",
      "Epoch:796,Loss : 0.6628251479541584\n",
      "Epoch:797,Loss : 0.6628247676661561\n",
      "Epoch:798,Loss : 0.6628243885560043\n",
      "Epoch:799,Loss : 0.6628240106194273\n",
      "Epoch:800,Loss : 0.6628236338521687\n",
      "Epoch:801,Loss : 0.6628232582499893\n",
      "Epoch:802,Loss : 0.6628228838086684\n",
      "Epoch:803,Loss : 0.6628225105240034\n",
      "Epoch:804,Loss : 0.6628221383918097\n",
      "Epoch:805,Loss : 0.6628217674079201\n",
      "Epoch:806,Loss : 0.6628213975681857\n",
      "Epoch:807,Loss : 0.6628210288684746\n",
      "Epoch:808,Loss : 0.6628206613046735\n",
      "Epoch:809,Loss : 0.6628202948726849\n",
      "Epoch:810,Loss : 0.6628199295684304\n",
      "Epoch:811,Loss : 0.6628195653878481\n",
      "Epoch:812,Loss : 0.6628192023268933\n",
      "Epoch:813,Loss : 0.6628188403815383\n",
      "Epoch:814,Loss : 0.6628184795477732\n",
      "Epoch:815,Loss : 0.6628181198216038\n",
      "Epoch:816,Loss : 0.6628177611990539\n",
      "Epoch:817,Loss : 0.6628174036761636\n",
      "Epoch:818,Loss : 0.6628170472489895\n",
      "Epoch:819,Loss : 0.6628166919136055\n",
      "Epoch:820,Loss : 0.6628163376661013\n",
      "Epoch:821,Loss : 0.6628159845025833\n",
      "Epoch:822,Loss : 0.6628156324191746\n",
      "Epoch:823,Loss : 0.6628152814120142\n",
      "Epoch:824,Loss : 0.6628149314772572\n",
      "Epoch:825,Loss : 0.6628145826110752\n",
      "Epoch:826,Loss : 0.6628142348096556\n",
      "Epoch:827,Loss : 0.6628138880692019\n",
      "Epoch:828,Loss : 0.6628135423859335\n",
      "Epoch:829,Loss : 0.6628131977560853\n",
      "Epoch:830,Loss : 0.6628128541759084\n",
      "Epoch:831,Loss : 0.6628125116416694\n",
      "Epoch:832,Loss : 0.6628121701496501\n",
      "Epoch:833,Loss : 0.6628118296961485\n",
      "Epoch:834,Loss : 0.6628114902774771\n",
      "Epoch:835,Loss : 0.6628111518899644\n",
      "Epoch:836,Loss : 0.6628108145299545\n",
      "Epoch:837,Loss : 0.6628104781938058\n",
      "Epoch:838,Loss : 0.6628101428778924\n",
      "Epoch:839,Loss : 0.662809808578603\n",
      "Epoch:840,Loss : 0.6628094752923422\n",
      "Epoch:841,Loss : 0.6628091430155284\n",
      "Epoch:842,Loss : 0.6628088117445953\n",
      "Epoch:843,Loss : 0.6628084814759914\n",
      "Epoch:844,Loss : 0.6628081522061803\n",
      "Epoch:845,Loss : 0.6628078239316392\n",
      "Epoch:846,Loss : 0.6628074966488604\n",
      "Epoch:847,Loss : 0.662807170354351\n",
      "Epoch:848,Loss : 0.662806845044632\n",
      "Epoch:849,Loss : 0.662806520716239\n",
      "Epoch:850,Loss : 0.6628061973657215\n",
      "Epoch:851,Loss : 0.6628058749896437\n",
      "Epoch:852,Loss : 0.6628055535845836\n",
      "Epoch:853,Loss : 0.6628052331471332\n",
      "Epoch:854,Loss : 0.662804913673899\n",
      "Epoch:855,Loss : 0.6628045951615007\n",
      "Epoch:856,Loss : 0.662804277606572\n",
      "Epoch:857,Loss : 0.662803961005761\n",
      "Epoch:858,Loss : 0.662803645355729\n",
      "Epoch:859,Loss : 0.6628033306531507\n",
      "Epoch:860,Loss : 0.662803016894715\n",
      "Epoch:861,Loss : 0.662802704077124\n",
      "Epoch:862,Loss : 0.6628023921970937\n",
      "Epoch:863,Loss : 0.6628020812513524\n",
      "Epoch:864,Loss : 0.6628017712366429\n",
      "Epoch:865,Loss : 0.6628014621497205\n",
      "Epoch:866,Loss : 0.662801153987354\n",
      "Epoch:867,Loss : 0.6628008467463258\n",
      "Epoch:868,Loss : 0.6628005404234303\n",
      "Epoch:869,Loss : 0.6628002350154757\n",
      "Epoch:870,Loss : 0.6627999305192833\n",
      "Epoch:871,Loss : 0.6627996269316866\n",
      "Epoch:872,Loss : 0.6627993242495325\n",
      "Epoch:873,Loss : 0.6627990224696803\n",
      "Epoch:874,Loss : 0.662798721589002\n",
      "Epoch:875,Loss : 0.6627984216043828\n",
      "Epoch:876,Loss : 0.6627981225127199\n",
      "Epoch:877,Loss : 0.662797824310923\n",
      "Epoch:878,Loss : 0.662797526995915\n",
      "Epoch:879,Loss : 0.6627972305646299\n",
      "Epoch:880,Loss : 0.6627969350140157\n",
      "Epoch:881,Loss : 0.6627966403410313\n",
      "Epoch:882,Loss : 0.6627963465426485\n",
      "Epoch:883,Loss : 0.6627960536158513\n",
      "Epoch:884,Loss : 0.6627957615576352\n",
      "Epoch:885,Loss : 0.6627954703650087\n",
      "Epoch:886,Loss : 0.6627951800349917\n",
      "Epoch:887,Loss : 0.6627948905646158\n",
      "Epoch:888,Loss : 0.6627946019509254\n",
      "Epoch:889,Loss : 0.6627943141909759\n",
      "Epoch:890,Loss : 0.6627940272818347\n",
      "Epoch:891,Loss : 0.6627937412205814\n",
      "Epoch:892,Loss : 0.6627934560043062\n",
      "Epoch:893,Loss : 0.6627931716301122\n",
      "Epoch:894,Loss : 0.6627928880951133\n",
      "Epoch:895,Loss : 0.6627926053964348\n",
      "Epoch:896,Loss : 0.662792323531214\n",
      "Epoch:897,Loss : 0.6627920424965992\n",
      "Epoch:898,Loss : 0.66279176228975\n",
      "Epoch:899,Loss : 0.6627914829078376\n",
      "Epoch:900,Loss : 0.6627912043480444\n",
      "Epoch:901,Loss : 0.6627909266075636\n",
      "Epoch:902,Loss : 0.6627906496836001\n",
      "Epoch:903,Loss : 0.6627903735733696\n",
      "Epoch:904,Loss : 0.6627900982740983\n",
      "Epoch:905,Loss : 0.6627898237830249\n",
      "Epoch:906,Loss : 0.6627895500973969\n",
      "Epoch:907,Loss : 0.6627892772144748\n",
      "Epoch:908,Loss : 0.6627890051315284\n",
      "Epoch:909,Loss : 0.662788733845839\n",
      "Epoch:910,Loss : 0.6627884633546987\n",
      "Epoch:911,Loss : 0.6627881936554099\n",
      "Epoch:912,Loss : 0.6627879247452855\n",
      "Epoch:913,Loss : 0.6627876566216497\n",
      "Epoch:914,Loss : 0.6627873892818367\n",
      "Epoch:915,Loss : 0.6627871227231912\n",
      "Epoch:916,Loss : 0.6627868569430685\n",
      "Epoch:917,Loss : 0.6627865919388343\n",
      "Epoch:918,Loss : 0.6627863277078644\n",
      "Epoch:919,Loss : 0.6627860642475452\n",
      "Epoch:920,Loss : 0.6627858015552731\n",
      "Epoch:921,Loss : 0.6627855396284551\n",
      "Epoch:922,Loss : 0.6627852784645077\n",
      "Epoch:923,Loss : 0.6627850180608582\n",
      "Epoch:924,Loss : 0.6627847584149436\n",
      "Epoch:925,Loss : 0.6627844995242106\n",
      "Epoch:926,Loss : 0.6627842413861166\n",
      "Epoch:927,Loss : 0.6627839839981284\n",
      "Epoch:928,Loss : 0.6627837273577231\n",
      "Epoch:929,Loss : 0.6627834714623871\n",
      "Epoch:930,Loss : 0.662783216309617\n",
      "Epoch:931,Loss : 0.6627829618969188\n",
      "Epoch:932,Loss : 0.6627827082218084\n",
      "Epoch:933,Loss : 0.6627824552818118\n",
      "Epoch:934,Loss : 0.6627822030744639\n",
      "Epoch:935,Loss : 0.6627819515973095\n",
      "Epoch:936,Loss : 0.6627817008479028\n",
      "Epoch:937,Loss : 0.6627814508238076\n",
      "Epoch:938,Loss : 0.6627812015225969\n",
      "Epoch:939,Loss : 0.6627809529418539\n",
      "Epoch:940,Loss : 0.6627807050791701\n",
      "Epoch:941,Loss : 0.6627804579321466\n",
      "Epoch:942,Loss : 0.6627802114983944\n",
      "Epoch:943,Loss : 0.6627799657755331\n",
      "Epoch:944,Loss : 0.6627797207611915\n",
      "Epoch:945,Loss : 0.6627794764530082\n",
      "Epoch:946,Loss : 0.6627792328486298\n",
      "Epoch:947,Loss : 0.662778989945713\n",
      "Epoch:948,Loss : 0.6627787477419231\n",
      "Epoch:949,Loss : 0.6627785062349341\n",
      "Epoch:950,Loss : 0.6627782654224295\n",
      "Epoch:951,Loss : 0.6627780253021013\n",
      "Epoch:952,Loss : 0.6627777858716506\n",
      "Epoch:953,Loss : 0.6627775471287873\n",
      "Epoch:954,Loss : 0.6627773090712298\n",
      "Epoch:955,Loss : 0.6627770716967057\n",
      "Epoch:956,Loss : 0.6627768350029509\n",
      "Epoch:957,Loss : 0.66277659898771\n",
      "Epoch:958,Loss : 0.6627763636487368\n",
      "Epoch:959,Loss : 0.6627761289837931\n",
      "Epoch:960,Loss : 0.6627758949906493\n",
      "Epoch:961,Loss : 0.6627756616670842\n",
      "Epoch:962,Loss : 0.6627754290108858\n",
      "Epoch:963,Loss : 0.6627751970198497\n",
      "Epoch:964,Loss : 0.6627749656917803\n",
      "Epoch:965,Loss : 0.6627747350244905\n",
      "Epoch:966,Loss : 0.6627745050158013\n",
      "Epoch:967,Loss : 0.6627742756635416\n",
      "Epoch:968,Loss : 0.6627740469655495\n",
      "Epoch:969,Loss : 0.6627738189196708\n",
      "Epoch:970,Loss : 0.6627735915237591\n",
      "Epoch:971,Loss : 0.6627733647756766\n",
      "Epoch:972,Loss : 0.6627731386732937\n",
      "Epoch:973,Loss : 0.6627729132144887\n",
      "Epoch:974,Loss : 0.662772688397148\n",
      "Epoch:975,Loss : 0.6627724642191657\n",
      "Epoch:976,Loss : 0.6627722406784445\n",
      "Epoch:977,Loss : 0.6627720177728941\n",
      "Epoch:978,Loss : 0.6627717955004332\n",
      "Epoch:979,Loss : 0.6627715738589876\n",
      "Epoch:980,Loss : 0.662771352846491\n",
      "Epoch:981,Loss : 0.6627711324608851\n",
      "Epoch:982,Loss : 0.6627709127001193\n",
      "Epoch:983,Loss : 0.6627706935621511\n",
      "Epoch:984,Loss : 0.6627704750449447\n",
      "Epoch:985,Loss : 0.6627702571464729\n",
      "Epoch:986,Loss : 0.6627700398647156\n",
      "Epoch:987,Loss : 0.6627698231976608\n",
      "Epoch:988,Loss : 0.6627696071433036\n",
      "Epoch:989,Loss : 0.6627693916996464\n",
      "Epoch:990,Loss : 0.6627691768647002\n",
      "Epoch:991,Loss : 0.6627689626364821\n",
      "Epoch:992,Loss : 0.6627687490130175\n",
      "Epoch:993,Loss : 0.6627685359923389\n",
      "Epoch:994,Loss : 0.6627683235724864\n",
      "Epoch:995,Loss : 0.6627681117515068\n",
      "Epoch:996,Loss : 0.662767900527455\n",
      "Epoch:997,Loss : 0.6627676898983926\n",
      "Epoch:998,Loss : 0.6627674798623892\n",
      "Epoch:999,Loss : 0.6627672704175199\n",
      "Epoch:1000,Loss : 0.6627670615618695\n"
     ]
    }
   ],
   "source": [
    "x =[]\n",
    "y =[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    \n",
    "    x.append(epoch)\n",
    "\n",
    "    y_pred = model.forward(X_train_tensor)\n",
    "\n",
    "\n",
    "    loss = model.loss_function(y_pred ,y_test_tensor)\n",
    "    y.append(loss)\n",
    "\n",
    "    print(f'Epoch:{epoch+1},Loss : {loss.item()}')\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "\n",
    "    model.weights.grad.zero_()\n",
    "\n",
    "    model.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fec9e7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3cbc22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3.7187, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(3.4229, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(3.1061, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(2.7588, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(2.3643, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(1.9568, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(1.5877, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(1.2970, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(1.0878, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.9628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.8996, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.8638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.8374, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.8154, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7963, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7799, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7657, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7536, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7434, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7347, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7273, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7211, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7159, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7114, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7076, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7043, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.7015, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6990, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6968, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6949, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6932, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6916, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6902, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6890, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6878, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6867, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6857, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6848, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6840, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6832, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6824, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6817, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6810, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6804, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6798, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6792, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6787, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6782, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6777, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6772, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6768, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6764, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6759, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6755, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6752, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6748, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6745, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6741, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6738, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6735, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6732, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6729, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6726, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6723, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6721, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6718, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6716, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6713, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6711, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6709, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6707, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6705, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6703, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6701, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6699, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6697, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6695, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6694, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6692, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6691, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6689, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6688, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6686, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6685, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6683, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6682, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6681, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6680, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6678, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6677, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6676, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6675, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6674, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6673, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6672, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6671, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6670, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6669, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6668, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6667, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6667, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6666, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6665, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6664, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6663, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6663, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6662, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6661, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6661, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6660, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6659, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6659, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6658, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6658, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6657, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6657, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6656, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6656, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6655, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6655, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6654, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6654, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6653, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6653, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6652, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6652, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6652, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6651, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6651, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6650, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6650, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6650, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6649, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6649, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6649, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6648, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6648, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6648, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6648, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6647, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6647, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6647, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6646, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6646, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6646, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6646, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6646, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6645, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6645, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6645, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6645, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6644, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6644, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6644, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6644, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6644, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6643, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6643, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6643, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6643, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6643, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6643, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6642, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6642, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6642, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6642, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6642, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6642, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6641, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6641, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6641, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6641, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6641, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6641, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6641, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6641, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6640, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6640, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6640, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6640, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6640, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6640, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6640, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6640, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6639, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6638, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6637, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6636, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6635, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6634, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6633, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6632, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6631, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6630, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6629, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>),\n",
       " tensor(0.6628, dtype=torch.float64, grad_fn=<NegBackward0>)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39a09acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [t.item() for t in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89edc553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.718703023079527,\n",
       " 3.422909204424197,\n",
       " 3.1061040888668905,\n",
       " 2.7587758566216865,\n",
       " 2.3642921785548894,\n",
       " 1.956753579956398,\n",
       " 1.5876546204537392,\n",
       " 1.297048312974071,\n",
       " 1.0877738295178283,\n",
       " 0.9628133788911177,\n",
       " 0.8995563467435035,\n",
       " 0.8637564842752475,\n",
       " 0.8374267581148488,\n",
       " 0.8153514282703195,\n",
       " 0.7963020266242521,\n",
       " 0.7798556569378108,\n",
       " 0.7657239659509534,\n",
       " 0.7536447191550107,\n",
       " 0.7433699573278842,\n",
       " 0.7346659149737657,\n",
       " 0.7273129617499173,\n",
       " 0.7211063622493197,\n",
       " 0.7158590194333325,\n",
       " 0.7114051208274336,\n",
       " 0.7076025708017912,\n",
       " 0.7043330691916092,\n",
       " 0.7015001355824212,\n",
       " 0.6990260343962805,\n",
       " 0.6968484604533487,\n",
       " 0.6949174915644611,\n",
       " 0.6931930119718234,\n",
       " 0.6916426323170531,\n",
       " 0.6902400515690648,\n",
       " 0.6889637821533219,\n",
       " 0.6877961614828575,\n",
       " 0.6867225846289473,\n",
       " 0.6857309061683304,\n",
       " 0.6848109711902378,\n",
       " 0.6839542451508901,\n",
       " 0.6831535197643059,\n",
       " 0.6824026777694933,\n",
       " 0.6816965036193869,\n",
       " 0.6810305302532359,\n",
       " 0.680400914425955,\n",
       " 0.6798043347907067,\n",
       " 0.6792379082232873,\n",
       " 0.6786991208540146,\n",
       " 0.6781857710179107,\n",
       " 0.6776959219070379,\n",
       " 0.6772278621533102,\n",
       " 0.6767800729175519,\n",
       " 0.6763512003341944,\n",
       " 0.6759400323779378,\n",
       " 0.6755454793917194,\n",
       " 0.6751665576541066,\n",
       " 0.6748023754760477,\n",
       " 0.6744521214074298,\n",
       " 0.6741150542074323,\n",
       " 0.6737904942926183,\n",
       " 0.6734778164257398,\n",
       " 0.6731764434484478,\n",
       " 0.6728858408941593,\n",
       " 0.6726055123445708,\n",
       " 0.672334995415814,\n",
       " 0.6720738582788615,\n",
       " 0.6718216966342175,\n",
       " 0.671578131073754,\n",
       " 0.6713428047731972,\n",
       " 0.6711153814676676,\n",
       " 0.6708955436700753,\n",
       " 0.6706829910983759,\n",
       " 0.670477439282874,\n",
       " 0.6702786183291147,\n",
       " 0.6700862718155541,\n",
       " 0.6699001558082701,\n",
       " 0.6697200379775686,\n",
       " 0.6695456968035204,\n",
       " 0.669376920859314,\n",
       " 0.669213508162873,\n",
       " 0.6690552655885121,\n",
       " 0.6689020083315373,\n",
       " 0.6687535594196498,\n",
       " 0.6686097492658384,\n",
       " 0.6684704152581402,\n",
       " 0.6683354013822527,\n",
       " 0.6682045578734861,\n",
       " 0.6680777408949976,\n",
       " 0.6679548122396213,\n",
       " 0.6678356390529344,\n",
       " 0.6677200935754922,\n",
       " 0.6676080529024013,\n",
       " 0.667499398758616,\n",
       " 0.6673940172885309,\n",
       " 0.6672917988585993,\n",
       " 0.6671926378718519,\n",
       " 0.6670964325933079,\n",
       " 0.667003084985386,\n",
       " 0.6669125005525081,\n",
       " 0.6668245881941796,\n",
       " 0.6667392600658985,\n",
       " 0.6666564314473111,\n",
       " 0.66657602061709,\n",
       " 0.6664979487340577,\n",
       " 0.6664221397241299,\n",
       " 0.6663485201726842,\n",
       " 0.6662770192220007,\n",
       " 0.6662075684734514,\n",
       " 0.6661401018941432,\n",
       " 0.6660745557277418,\n",
       " 0.6660108684092316,\n",
       " 0.6659489804833826,\n",
       " 0.665888834526713,\n",
       " 0.665830375072758,\n",
       " 0.6657735485404609,\n",
       " 0.6657183031655272,\n",
       " 0.6656645889345818,\n",
       " 0.665612357521993,\n",
       " 0.665561562229225,\n",
       " 0.6655121579265995,\n",
       " 0.6654641009973479,\n",
       " 0.6654173492838489,\n",
       " 0.6653718620359454,\n",
       " 0.6653275998612524,\n",
       " 0.66528452467736,\n",
       " 0.6652425996658505,\n",
       " 0.6652017892280517,\n",
       " 0.6651620589424494,\n",
       " 0.6651233755236858,\n",
       " 0.6650857067830852,\n",
       " 0.6650490215906342,\n",
       " 0.6650132898383626,\n",
       " 0.6649784824050651,\n",
       " 0.664944571122311,\n",
       " 0.6649115287416889,\n",
       " 0.6648793289032392,\n",
       " 0.6648479461050261,\n",
       " 0.6648173556738041,\n",
       " 0.6647875337367395,\n",
       " 0.6647584571941422,\n",
       " 0.6647301036931725,\n",
       " 0.6647024516024821,\n",
       " 0.6646754799877588,\n",
       " 0.6646491685881354,\n",
       " 0.6646234977934344,\n",
       " 0.6645984486222158,\n",
       " 0.6645740027005965,\n",
       " 0.6645501422418167,\n",
       " 0.6645268500265211,\n",
       " 0.6645041093837321,\n",
       " 0.6644819041724873,\n",
       " 0.6644602187641192,\n",
       " 0.6644390380251507,\n",
       " 0.6644183473007874,\n",
       " 0.6643981323989824,\n",
       " 0.6643783795750536,\n",
       " 0.6643590755168353,\n",
       " 0.6643402073303412,\n",
       " 0.6643217625259242,\n",
       " 0.6643037290049131,\n",
       " 0.6642860950467087,\n",
       " 0.6642688492963252,\n",
       " 0.6642519807523585,\n",
       " 0.6642354787553668,\n",
       " 0.6642193329766509,\n",
       " 0.664203533407416,\n",
       " 0.6641880703483065,\n",
       " 0.6641729343992957,\n",
       " 0.6641581164499206,\n",
       " 0.6641436076698503,\n",
       " 0.6641293994997729,\n",
       " 0.6641154836425939,\n",
       " 0.6641018520549308,\n",
       " 0.6640884969388978,\n",
       " 0.6640754107341674,\n",
       " 0.6640625861102997,\n",
       " 0.6640500159593308,\n",
       " 0.6640376933886117,\n",
       " 0.6640256117138869,\n",
       " 0.6640137644526056,\n",
       " 0.6640021453174586,\n",
       " 0.6639907482101309,\n",
       " 0.6639795672152635,\n",
       " 0.663968596594617,\n",
       " 0.66395783078143,\n",
       " 0.6639472643749661,\n",
       " 0.6639368921352427,\n",
       " 0.6639267089779324,\n",
       " 0.6639167099694383,\n",
       " 0.6639068903221288,\n",
       " 0.6638972453897317,\n",
       " 0.6638877706628808,\n",
       " 0.6638784617648092,\n",
       " 0.6638693144471846,\n",
       " 0.6638603245860824,\n",
       " 0.6638514881780901,\n",
       " 0.6638428013365416,\n",
       " 0.6638342602878724,\n",
       " 0.6638258613680971,\n",
       " 0.6638176010193998,\n",
       " 0.6638094757868368,\n",
       " 0.6638014823151479,\n",
       " 0.6637936173456689,\n",
       " 0.6637858777133482,\n",
       " 0.6637782603438561,\n",
       " 0.6637707622507927,\n",
       " 0.6637633805329812,\n",
       " 0.6637561123718531,\n",
       " 0.6637489550289146,\n",
       " 0.6637419058432975,\n",
       " 0.6637349622293867,\n",
       " 0.6637281216745246,\n",
       " 0.6637213817367908,\n",
       " 0.6637147400428517,\n",
       " 0.6637081942858768,\n",
       " 0.6637017422235271,\n",
       " 0.6636953816760021,\n",
       " 0.6636891105241529,\n",
       " 0.663682926707653,\n",
       " 0.6636768282232292,\n",
       " 0.6636708131229463,\n",
       " 0.663664879512549,\n",
       " 0.6636590255498546,\n",
       " 0.6636532494431951,\n",
       " 0.6636475494499131,\n",
       " 0.6636419238748987,\n",
       " 0.6636363710691787,\n",
       " 0.6636308894285452,\n",
       " 0.6636254773922311,\n",
       " 0.6636201334416232,\n",
       " 0.6636148560990197,\n",
       " 0.6636096439264231,\n",
       " 0.6636044955243721,\n",
       " 0.6635994095308106,\n",
       " 0.6635943846199907,\n",
       " 0.6635894195014096,\n",
       " 0.6635845129187811,\n",
       " 0.6635796636490369,\n",
       " 0.6635748705013603,\n",
       " 0.6635701323162487,\n",
       " 0.6635654479646057,\n",
       " 0.6635608163468603,\n",
       " 0.6635562363921147,\n",
       " 0.6635517070573163,\n",
       " 0.663547227326456,\n",
       " 0.6635427962097921,\n",
       " 0.6635384127430951,\n",
       " 0.6635340759869182,\n",
       " 0.6635297850258893,\n",
       " 0.6635255389680236,\n",
       " 0.6635213369440573,\n",
       " 0.6635171781068034,\n",
       " 0.6635130616305241,\n",
       " 0.6635089867103234,\n",
       " 0.6635049525615591,\n",
       " 0.6635009584192713,\n",
       " 0.6634970035376274,\n",
       " 0.6634930871893859,\n",
       " 0.6634892086653745,\n",
       " 0.6634853672739841,\n",
       " 0.6634815623406787,\n",
       " 0.6634777932075191,\n",
       " 0.6634740592327011,\n",
       " 0.6634703597901073,\n",
       " 0.6634666942688734,\n",
       " 0.6634630620729646,\n",
       " 0.6634594626207672,\n",
       " 0.6634558953446915,\n",
       " 0.6634523596907862,\n",
       " 0.6634488551183634,\n",
       " 0.6634453810996361,\n",
       " 0.6634419371193657,\n",
       " 0.6634385226745196,\n",
       " 0.6634351372739392,\n",
       " 0.6634317804380172,\n",
       " 0.6634284516983846,\n",
       " 0.6634251505976074,\n",
       " 0.6634218766888907,\n",
       " 0.6634186295357926,\n",
       " 0.6634154087119458,\n",
       " 0.6634122138007879,\n",
       " 0.6634090443952987,\n",
       " 0.6634059000977454,\n",
       " 0.6634027805194356,\n",
       " 0.6633996852804765,\n",
       " 0.6633966140095422,\n",
       " 0.6633935663436463,\n",
       " 0.6633905419279221,\n",
       " 0.6633875404154084,\n",
       " 0.6633845614668424,\n",
       " 0.6633816047504566,\n",
       " 0.6633786699417844,\n",
       " 0.6633757567234676,\n",
       " 0.663372864785073,\n",
       " 0.6633699938229112,\n",
       " 0.6633671435398626,\n",
       " 0.6633643136452068,\n",
       " 0.6633615038544586,\n",
       " 0.663358713889206,\n",
       " 0.6633559434769558,\n",
       " 0.663353192350981,\n",
       " 0.6633504602501737,\n",
       " 0.6633477469189026,\n",
       " 0.663345052106872,\n",
       " 0.6633423755689888,\n",
       " 0.6633397170652288,\n",
       " 0.6633370763605103,\n",
       " 0.6633344532245689,\n",
       " 0.6633318474318367,\n",
       " 0.6633292587613245,\n",
       " 0.6633266869965089,\n",
       " 0.6633241319252179,\n",
       " 0.6633215933395263,\n",
       " 0.6633190710356488,\n",
       " 0.6633165648138368,\n",
       " 0.663314074478281,\n",
       " 0.6633115998370132,\n",
       " 0.6633091407018117,\n",
       " 0.6633066968881113,\n",
       " 0.6633042682149132,\n",
       " 0.6633018545046978,\n",
       " 0.6632994555833418,\n",
       " 0.6632970712800346,\n",
       " 0.6632947014271992,\n",
       " 0.6632923458604146,\n",
       " 0.6632900044183395,\n",
       " 0.6632876769426396,\n",
       " 0.6632853632779151,\n",
       " 0.6632830632716314,\n",
       " 0.6632807767740512,\n",
       " 0.6632785036381684,\n",
       " 0.6632762437196438,\n",
       " 0.6632739968767424,\n",
       " 0.6632717629702727,\n",
       " 0.6632695418635268,\n",
       " 0.6632673334222233,\n",
       " 0.6632651375144507,\n",
       " 0.6632629540106125,\n",
       " 0.6632607827833741,\n",
       " 0.6632586237076109,\n",
       " 0.6632564766603573,\n",
       " 0.6632543415207576,\n",
       " 0.6632522181700181,\n",
       " 0.6632501064913606,\n",
       " 0.6632480063699766,\n",
       " 0.6632459176929821,\n",
       " 0.6632438403493762,\n",
       " 0.6632417742299973,\n",
       " 0.6632397192274824,\n",
       " 0.663237675236229,\n",
       " 0.6632356421523526,\n",
       " 0.6632336198736521,\n",
       " 0.6632316082995706,\n",
       " 0.6632296073311609,\n",
       " 0.6632276168710488,\n",
       " 0.6632256368234006,\n",
       " 0.6632236670938879,\n",
       " 0.6632217075896567,\n",
       " 0.663219758219294,\n",
       " 0.6632178188927987,\n",
       " 0.66321588952155,\n",
       " 0.6632139700182784,\n",
       " 0.6632120602970374,\n",
       " 0.6632101602731754,\n",
       " 0.6632082698633076,\n",
       " 0.6632063889852914,\n",
       " 0.6632045175581973,\n",
       " 0.6632026555022873,\n",
       " 0.6632008027389872,\n",
       " 0.6631989591908645,\n",
       " 0.6631971247816036,\n",
       " 0.6631952994359837,\n",
       " 0.6631934830798559,\n",
       " 0.6631916756401224,\n",
       " 0.6631898770447134,\n",
       " 0.6631880872225685,\n",
       " 0.6631863061036147,\n",
       " 0.6631845336187477,\n",
       " 0.6631827696998119,\n",
       " 0.6631810142795819,\n",
       " 0.6631792672917443,\n",
       " 0.6631775286708792,\n",
       " 0.6631757983524433,\n",
       " 0.6631740762727523,\n",
       " 0.6631723623689645,\n",
       " 0.6631706565790642,\n",
       " 0.663168958841846,\n",
       " 0.6631672690968994,\n",
       " 0.663165587284593,\n",
       " 0.6631639133460606,\n",
       " 0.6631622472231855,\n",
       " 0.6631605888585878,\n",
       " 0.6631589381956089,\n",
       " 0.6631572951782998,\n",
       " 0.6631556597514066,\n",
       " 0.6631540318603577,\n",
       " 0.663152411451252,\n",
       " 0.6631507984708459,\n",
       " 0.6631491928665412,\n",
       " 0.663147594586374,\n",
       " 0.6631460035790028,\n",
       " 0.6631444197936973,\n",
       " 0.663142843180327,\n",
       " 0.6631412736893516,\n",
       " 0.6631397112718092,\n",
       " 0.6631381558793068,\n",
       " 0.6631366074640105,\n",
       " 0.6631350659786344,\n",
       " 0.6631335313764332,\n",
       " 0.6631320036111903,\n",
       " 0.6631304826372111,\n",
       " 0.6631289684093113,\n",
       " 0.6631274608828115,\n",
       " 0.6631259600135253,\n",
       " 0.6631244657577527,\n",
       " 0.663122978072272,\n",
       " 0.6631214969143308,\n",
       " 0.6631200222416391,\n",
       " 0.6631185540123611,\n",
       " 0.663117092185107,\n",
       " 0.6631156367189276,\n",
       " 0.663114187573305,\n",
       " 0.6631127447081467,\n",
       " 0.6631113080837784,\n",
       " 0.6631098776609372,\n",
       " 0.6631084534007649,\n",
       " 0.6631070352648019,\n",
       " 0.6631056232149803,\n",
       " 0.6631042172136183,\n",
       " 0.6631028172234134,\n",
       " 0.6631014232074373,\n",
       " 0.6631000351291289,\n",
       " 0.6630986529522904,\n",
       " 0.6630972766410795,\n",
       " 0.6630959061600054,\n",
       " 0.6630945414739229,\n",
       " 0.6630931825480274,\n",
       " 0.6630918293478499,\n",
       " 0.663090481839251,\n",
       " 0.6630891399884169,\n",
       " 0.6630878037618545,\n",
       " 0.6630864731263857,\n",
       " 0.6630851480491441,\n",
       " 0.663083828497569,\n",
       " 0.663082514439403,\n",
       " 0.6630812058426845,\n",
       " 0.6630799026757466,\n",
       " 0.6630786049072104,\n",
       " 0.6630773125059827,\n",
       " 0.6630760254412508,\n",
       " 0.663074743682479,\n",
       " 0.6630734671994037,\n",
       " 0.6630721959620318,\n",
       " 0.6630709299406349,\n",
       " 0.663069669105746,\n",
       " 0.6630684134281563,\n",
       " 0.6630671628789115,\n",
       " 0.6630659174293091,\n",
       " 0.663064677050893,\n",
       " 0.6630634417154518,\n",
       " 0.6630622113950156,\n",
       " 0.6630609860618518,\n",
       " 0.6630597656884617,\n",
       " 0.6630585502475792,\n",
       " 0.6630573397121663,\n",
       " 0.6630561340554099,\n",
       " 0.6630549332507196,\n",
       " 0.6630537372717249,\n",
       " 0.6630525460922718,\n",
       " 0.66305135968642,\n",
       " 0.6630501780284408,\n",
       " 0.6630490010928137,\n",
       " 0.6630478288542242,\n",
       " 0.6630466612875611,\n",
       " 0.663045498367914,\n",
       " 0.6630443400705708,\n",
       " 0.6630431863710143,\n",
       " 0.6630420372449222,\n",
       " 0.6630408926681617,\n",
       " 0.6630397526167897,\n",
       " 0.6630386170670486,\n",
       " 0.6630374859953655,\n",
       " 0.6630363593783494,\n",
       " 0.6630352371927885,\n",
       " 0.6630341194156484,\n",
       " 0.6630330060240712,\n",
       " 0.6630318969953718,\n",
       " 0.663030792307036,\n",
       " 0.6630296919367192,\n",
       " 0.6630285958622448,\n",
       " 0.6630275040616007,\n",
       " 0.663026416512939,\n",
       " 0.663025333194573,\n",
       " 0.6630242540849762,\n",
       " 0.6630231791627801,\n",
       " 0.6630221084067719,\n",
       " 0.6630210417958938,\n",
       " 0.6630199793092405,\n",
       " 0.663018920926058,\n",
       " 0.6630178666257414,\n",
       " 0.6630168163878333,\n",
       " 0.6630157701920232,\n",
       " 0.6630147280181443,\n",
       " 0.6630136898461734,\n",
       " 0.663012655656228,\n",
       " 0.6630116254285664,\n",
       " 0.663010599143584,\n",
       " 0.6630095767818147,\n",
       " 0.6630085583239265,\n",
       " 0.663007543750722,\n",
       " 0.6630065330431367,\n",
       " 0.6630055261822368,\n",
       " 0.6630045231492188,\n",
       " 0.6630035239254074,\n",
       " 0.6630025284922546,\n",
       " 0.6630015368313386,\n",
       " 0.6630005489243618,\n",
       " 0.6629995647531503,\n",
       " 0.662998584299652,\n",
       " 0.6629976075459358,\n",
       " 0.6629966344741907,\n",
       " 0.6629956650667234,\n",
       " 0.6629946993059583,\n",
       " 0.662993737174436,\n",
       " 0.6629927786548121,\n",
       " 0.6629918237298558,\n",
       " 0.6629908723824495,\n",
       " 0.6629899245955867,\n",
       " 0.662988980352372,\n",
       " 0.6629880396360193,\n",
       " 0.6629871024298507,\n",
       " 0.6629861687172964,\n",
       " 0.6629852384818926,\n",
       " 0.6629843117072808,\n",
       " 0.6629833883772073,\n",
       " 0.6629824684755214,\n",
       " 0.6629815519861753,\n",
       " 0.6629806388932222,\n",
       " 0.6629797291808168,\n",
       " 0.6629788228332121,\n",
       " 0.6629779198347613,\n",
       " 0.6629770201699143,\n",
       " 0.6629761238232185,\n",
       " 0.6629752307793175,\n",
       " 0.6629743410229496,\n",
       " 0.6629734545389478,\n",
       " 0.6629725713122387,\n",
       " 0.6629716913278414,\n",
       " 0.6629708145708667,\n",
       " 0.6629699410265166,\n",
       " 0.6629690706800835,\n",
       " 0.662968203516949,\n",
       " 0.6629673395225835,\n",
       " 0.6629664786825452,\n",
       " 0.6629656209824792,\n",
       " 0.662964766408118,\n",
       " 0.6629639149452784,\n",
       " 0.6629630665798627,\n",
       " 0.662962221297858,\n",
       " 0.6629613790853333,\n",
       " 0.6629605399284421,\n",
       " 0.6629597038134188,\n",
       " 0.6629588707265796,\n",
       " 0.6629580406543214,\n",
       " 0.662957213583121,\n",
       " 0.6629563894995345,\n",
       " 0.6629555683901971,\n",
       " 0.6629547502418218,\n",
       " 0.6629539350411985,\n",
       " 0.6629531227751947,\n",
       " 0.6629523134307536,\n",
       " 0.662951506994894,\n",
       " 0.6629507034547096,\n",
       " 0.6629499027973688,\n",
       " 0.6629491050101131,\n",
       " 0.6629483100802572,\n",
       " 0.6629475179951891,\n",
       " 0.6629467287423676,\n",
       " 0.6629459423093241,\n",
       " 0.66294515868366,\n",
       " 0.662944377853047,\n",
       " 0.6629435998052272,\n",
       " 0.6629428245280112,\n",
       " 0.6629420520092784,\n",
       " 0.6629412822369767,\n",
       " 0.6629405151991211,\n",
       " 0.6629397508837936,\n",
       " 0.6629389892791436,\n",
       " 0.6629382303733856,\n",
       " 0.6629374741547999,\n",
       " 0.6629367206117319,\n",
       " 0.6629359697325918,\n",
       " 0.6629352215058534,\n",
       " 0.6629344759200543,\n",
       " 0.6629337329637951,\n",
       " 0.6629329926257393,\n",
       " 0.6629322548946124,\n",
       " 0.6629315197592013,\n",
       " 0.6629307872083545,\n",
       " 0.6629300572309814,\n",
       " 0.6629293298160512,\n",
       " 0.6629286049525936,\n",
       " 0.6629278826296973,\n",
       " 0.66292716283651,\n",
       " 0.6629264455622386,\n",
       " 0.6629257307961475,\n",
       " 0.6629250185275591,\n",
       " 0.6629243087458532,\n",
       " 0.6629236014404665,\n",
       " 0.6629228966008921,\n",
       " 0.6629221942166793,\n",
       " 0.6629214942774332,\n",
       " 0.6629207967728136,\n",
       " 0.6629201016925362,\n",
       " 0.6629194090263705,\n",
       " 0.6629187187641405,\n",
       " 0.662918030895724,\n",
       " 0.6629173454110517,\n",
       " 0.6629166623001077,\n",
       " 0.6629159815529289,\n",
       " 0.6629153031596045,\n",
       " 0.6629146271102748,\n",
       " 0.6629139533951327,\n",
       " 0.6629132820044216,\n",
       " 0.6629126129284363,\n",
       " 0.6629119461575218,\n",
       " 0.6629112816820733,\n",
       " 0.6629106194925355,\n",
       " 0.6629099595794031,\n",
       " 0.6629093019332201,\n",
       " 0.6629086465445784,\n",
       " 0.6629079934041192,\n",
       " 0.6629073425025316,\n",
       " 0.6629066938305523,\n",
       " 0.6629060473789661,\n",
       " 0.6629054031386045,\n",
       " 0.6629047611003457,\n",
       " 0.6629041212551152,\n",
       " 0.6629034835938842,\n",
       " 0.6629028481076699,\n",
       " 0.6629022147875354,\n",
       " 0.6629015836245891,\n",
       " 0.6629009546099839,\n",
       " 0.6629003277349186,\n",
       " 0.6628997029906352,\n",
       " 0.6628990803684206,\n",
       " 0.6628984598596056,\n",
       " 0.6628978414555646,\n",
       " 0.6628972251477151,\n",
       " 0.6628966109275175,\n",
       " 0.6628959987864754,\n",
       " 0.6628953887161345,\n",
       " 0.6628947807080832,\n",
       " 0.6628941747539514,\n",
       " 0.6628935708454108,\n",
       " 0.6628929689741748,\n",
       " 0.6628923691319976,\n",
       " 0.6628917713106741,\n",
       " 0.6628911755020405,\n",
       " 0.662890581697973,\n",
       " 0.662889989890388,\n",
       " 0.6628894000712415,\n",
       " 0.6628888122325297,\n",
       " 0.6628882263662873,\n",
       " 0.6628876424645889,\n",
       " 0.662887060519548,\n",
       " 0.6628864805233162,\n",
       " 0.6628859024680837,\n",
       " 0.6628853263460791,\n",
       " 0.6628847521495687,\n",
       " 0.6628841798708565,\n",
       " 0.6628836095022844,\n",
       " 0.6628830410362307,\n",
       " 0.6628824744651115,\n",
       " 0.6628819097813792,\n",
       " 0.662881346977523,\n",
       " 0.6628807860460683,\n",
       " 0.6628802269795764,\n",
       " 0.662879669770645,\n",
       " 0.6628791144119075,\n",
       " 0.6628785608960319,\n",
       " 0.6628780092157225,\n",
       " 0.6628774593637177,\n",
       " 0.6628769113327917,\n",
       " 0.6628763651157527,\n",
       " 0.662875820705443,\n",
       " 0.6628752780947398,\n",
       " 0.6628747372765541,\n",
       " 0.6628741982438303,\n",
       " 0.662873660989547,\n",
       " 0.6628731255067157,\n",
       " 0.6628725917883814,\n",
       " 0.6628720598276219,\n",
       " 0.6628715296175479,\n",
       " 0.6628710011513028,\n",
       " 0.6628704744220624,\n",
       " 0.6628699494230343,\n",
       " 0.6628694261474587,\n",
       " 0.6628689045886078,\n",
       " 0.6628683847397847,\n",
       " 0.6628678665943245,\n",
       " 0.6628673501455937,\n",
       " 0.6628668353869898,\n",
       " 0.6628663223119406,\n",
       " 0.6628658109139057,\n",
       " 0.6628653011863751,\n",
       " 0.6628647931228683,\n",
       " 0.662864286716936,\n",
       " 0.6628637819621587,\n",
       " 0.6628632788521462,\n",
       " 0.6628627773805393,\n",
       " 0.6628622775410071,\n",
       " 0.6628617793272485,\n",
       " 0.6628612827329922,\n",
       " 0.6628607877519945,\n",
       " 0.6628602943780422,\n",
       " 0.6628598026049494,\n",
       " 0.6628593124265603,\n",
       " 0.6628588238367458,\n",
       " 0.6628583368294062,\n",
       " 0.6628578513984699,\n",
       " 0.6628573675378922,\n",
       " 0.6628568852416569,\n",
       " 0.6628564045037756,\n",
       " 0.6628559253182867,\n",
       " 0.6628554476792564,\n",
       " 0.662854971580778,\n",
       " 0.6628544970169713,\n",
       " 0.6628540239819835,\n",
       " 0.6628535524699881,\n",
       " 0.6628530824751854,\n",
       " 0.662852613991802,\n",
       " 0.6628521470140907,\n",
       " 0.6628516815363306,\n",
       " 0.6628512175528263,\n",
       " 0.6628507550579087,\n",
       " 0.6628502940459342,\n",
       " 0.6628498345112845,\n",
       " 0.6628493764483672,\n",
       " 0.6628489198516145,\n",
       " 0.6628484647154846,\n",
       " 0.6628480110344595,\n",
       " 0.662847558803047,\n",
       " 0.6628471080157793,\n",
       " 0.6628466586672129,\n",
       " 0.6628462107519293,\n",
       " 0.6628457642645339,\n",
       " 0.662845319199656,\n",
       " 0.6628448755519496,\n",
       " 0.6628444333160926,\n",
       " 0.6628439924867856,\n",
       " 0.6628435530587542,\n",
       " 0.6628431150267468,\n",
       " 0.6628426783855352,\n",
       " 0.6628422431299149,\n",
       " 0.6628418092547043,\n",
       " 0.6628413767547443,\n",
       " 0.6628409456248996,\n",
       " 0.6628405158600572,\n",
       " 0.6628400874551269,\n",
       " 0.6628396604050407,\n",
       " 0.6628392347047535,\n",
       " 0.6628388103492422,\n",
       " 0.6628383873335058,\n",
       " 0.6628379656525657,\n",
       " 0.6628375453014652,\n",
       " 0.6628371262752687,\n",
       " 0.6628367085690634,\n",
       " 0.6628362921779576,\n",
       " 0.6628358770970805,\n",
       " 0.6628354633215837,\n",
       " 0.6628350508466395,\n",
       " 0.6628346396674417,\n",
       " 0.6628342297792043,\n",
       " 0.6628338211771632,\n",
       " 0.6628334138565743,\n",
       " 0.6628330078127153,\n",
       " 0.6628326030408831,\n",
       " 0.6628321995363964,\n",
       " 0.6628317972945931,\n",
       " 0.6628313963108321,\n",
       " 0.6628309965804926,\n",
       " 0.6628305980989736,\n",
       " 0.6628302008616939,\n",
       " 0.662829804864092,\n",
       " 0.6628294101016272,\n",
       " 0.6628290165697771,\n",
       " 0.6628286242640398,\n",
       " 0.6628282331799322,\n",
       " 0.6628278433129914,\n",
       " 0.6628274546587728,\n",
       " 0.6628270672128516,\n",
       " 0.6628266809708216,\n",
       " 0.6628262959282961,\n",
       " 0.6628259120809069,\n",
       " 0.6628255294243046,\n",
       " 0.6628251479541584,\n",
       " 0.6628247676661561,\n",
       " 0.6628243885560043,\n",
       " 0.6628240106194273,\n",
       " 0.6628236338521687,\n",
       " 0.6628232582499893,\n",
       " 0.6628228838086684,\n",
       " 0.6628225105240034,\n",
       " 0.6628221383918097,\n",
       " 0.6628217674079201,\n",
       " 0.6628213975681857,\n",
       " 0.6628210288684746,\n",
       " 0.6628206613046735,\n",
       " 0.6628202948726849,\n",
       " 0.6628199295684304,\n",
       " 0.6628195653878481,\n",
       " 0.6628192023268933,\n",
       " 0.6628188403815383,\n",
       " 0.6628184795477732,\n",
       " 0.6628181198216038,\n",
       " 0.6628177611990539,\n",
       " 0.6628174036761636,\n",
       " 0.6628170472489895,\n",
       " 0.6628166919136055,\n",
       " 0.6628163376661013,\n",
       " 0.6628159845025833,\n",
       " 0.6628156324191746,\n",
       " 0.6628152814120142,\n",
       " 0.6628149314772572,\n",
       " 0.6628145826110752,\n",
       " 0.6628142348096556,\n",
       " 0.6628138880692019,\n",
       " 0.6628135423859335,\n",
       " 0.6628131977560853,\n",
       " 0.6628128541759084,\n",
       " 0.6628125116416694,\n",
       " 0.6628121701496501,\n",
       " 0.6628118296961485,\n",
       " 0.6628114902774771,\n",
       " 0.6628111518899644,\n",
       " 0.6628108145299545,\n",
       " 0.6628104781938058,\n",
       " 0.6628101428778924,\n",
       " 0.662809808578603,\n",
       " 0.6628094752923422,\n",
       " 0.6628091430155284,\n",
       " 0.6628088117445953,\n",
       " 0.6628084814759914,\n",
       " 0.6628081522061803,\n",
       " 0.6628078239316392,\n",
       " 0.6628074966488604,\n",
       " 0.662807170354351,\n",
       " 0.662806845044632,\n",
       " 0.662806520716239,\n",
       " 0.6628061973657215,\n",
       " 0.6628058749896437,\n",
       " 0.6628055535845836,\n",
       " 0.6628052331471332,\n",
       " 0.662804913673899,\n",
       " 0.6628045951615007,\n",
       " 0.662804277606572,\n",
       " 0.662803961005761,\n",
       " 0.662803645355729,\n",
       " 0.6628033306531507,\n",
       " 0.662803016894715,\n",
       " 0.662802704077124,\n",
       " 0.6628023921970937,\n",
       " 0.6628020812513524,\n",
       " 0.6628017712366429,\n",
       " 0.6628014621497205,\n",
       " 0.662801153987354,\n",
       " 0.6628008467463258,\n",
       " 0.6628005404234303,\n",
       " 0.6628002350154757,\n",
       " 0.6627999305192833,\n",
       " 0.6627996269316866,\n",
       " 0.6627993242495325,\n",
       " 0.6627990224696803,\n",
       " 0.662798721589002,\n",
       " 0.6627984216043828,\n",
       " 0.6627981225127199,\n",
       " 0.662797824310923,\n",
       " 0.662797526995915,\n",
       " 0.6627972305646299,\n",
       " 0.6627969350140157,\n",
       " 0.6627966403410313,\n",
       " 0.6627963465426485,\n",
       " 0.6627960536158513,\n",
       " 0.6627957615576352,\n",
       " 0.6627954703650087,\n",
       " 0.6627951800349917,\n",
       " 0.6627948905646158,\n",
       " 0.6627946019509254,\n",
       " 0.6627943141909759,\n",
       " 0.6627940272818347,\n",
       " 0.6627937412205814,\n",
       " 0.6627934560043062,\n",
       " 0.6627931716301122,\n",
       " 0.6627928880951133,\n",
       " 0.6627926053964348,\n",
       " 0.662792323531214,\n",
       " 0.6627920424965992,\n",
       " 0.66279176228975,\n",
       " 0.6627914829078376,\n",
       " 0.6627912043480444,\n",
       " 0.6627909266075636,\n",
       " 0.6627906496836001,\n",
       " 0.6627903735733696,\n",
       " 0.6627900982740983,\n",
       " 0.6627898237830249,\n",
       " 0.6627895500973969,\n",
       " 0.6627892772144748,\n",
       " 0.6627890051315284,\n",
       " 0.662788733845839,\n",
       " 0.6627884633546987,\n",
       " 0.6627881936554099,\n",
       " 0.6627879247452855,\n",
       " 0.6627876566216497,\n",
       " 0.6627873892818367,\n",
       " 0.6627871227231912,\n",
       " 0.6627868569430685,\n",
       " 0.6627865919388343,\n",
       " 0.6627863277078644,\n",
       " 0.6627860642475452,\n",
       " 0.6627858015552731,\n",
       " 0.6627855396284551,\n",
       " 0.6627852784645077,\n",
       " 0.6627850180608582,\n",
       " 0.6627847584149436,\n",
       " 0.6627844995242106,\n",
       " 0.6627842413861166,\n",
       " 0.6627839839981284,\n",
       " 0.6627837273577231,\n",
       " 0.6627834714623871,\n",
       " 0.662783216309617,\n",
       " 0.6627829618969188,\n",
       " 0.6627827082218084,\n",
       " 0.6627824552818118,\n",
       " 0.6627822030744639,\n",
       " 0.6627819515973095,\n",
       " 0.6627817008479028,\n",
       " 0.6627814508238076,\n",
       " 0.6627812015225969,\n",
       " 0.6627809529418539,\n",
       " 0.6627807050791701,\n",
       " 0.6627804579321466,\n",
       " 0.6627802114983944,\n",
       " 0.6627799657755331,\n",
       " 0.6627797207611915,\n",
       " 0.6627794764530082,\n",
       " 0.6627792328486298,\n",
       " 0.662778989945713,\n",
       " 0.6627787477419231,\n",
       " 0.6627785062349341,\n",
       " 0.6627782654224295,\n",
       " 0.6627780253021013,\n",
       " 0.6627777858716506,\n",
       " 0.6627775471287873,\n",
       " 0.6627773090712298,\n",
       " 0.6627770716967057,\n",
       " 0.6627768350029509,\n",
       " 0.66277659898771,\n",
       " 0.6627763636487368,\n",
       " 0.6627761289837931,\n",
       " 0.6627758949906493,\n",
       " 0.6627756616670842,\n",
       " 0.6627754290108858,\n",
       " 0.6627751970198497,\n",
       " 0.6627749656917803,\n",
       " 0.6627747350244905,\n",
       " 0.6627745050158013,\n",
       " 0.6627742756635416,\n",
       " 0.6627740469655495,\n",
       " 0.6627738189196708,\n",
       " 0.6627735915237591,\n",
       " 0.6627733647756766,\n",
       " 0.6627731386732937,\n",
       " 0.6627729132144887,\n",
       " 0.662772688397148,\n",
       " 0.6627724642191657,\n",
       " 0.6627722406784445,\n",
       " 0.6627720177728941,\n",
       " 0.6627717955004332,\n",
       " 0.6627715738589876,\n",
       " 0.662771352846491,\n",
       " 0.6627711324608851,\n",
       " 0.6627709127001193,\n",
       " 0.6627706935621511,\n",
       " 0.6627704750449447,\n",
       " 0.6627702571464729,\n",
       " 0.6627700398647156,\n",
       " 0.6627698231976608,\n",
       " 0.6627696071433036,\n",
       " 0.6627693916996464,\n",
       " 0.6627691768647002,\n",
       " 0.6627689626364821,\n",
       " 0.6627687490130175,\n",
       " 0.6627685359923389,\n",
       " 0.6627683235724864,\n",
       " 0.6627681117515068,\n",
       " 0.662767900527455,\n",
       " 0.6627676898983926,\n",
       " 0.6627674798623892,\n",
       " 0.6627672704175199,\n",
       " 0.6627670615618695]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b75a3401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\learning\\pytorch\\.venv\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\learning\\pytorch\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ba1c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20ebac0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cd94322680>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, values, label=\"y vs x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c7d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
